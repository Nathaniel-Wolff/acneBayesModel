{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dfa384b",
   "metadata": {},
   "source": [
    "The purpose of the notebook is the following:\n",
    "\n",
    "0) Receive a dataset containing an aggregate dataframe with acne severities for a group of patients\n",
    "1) Parse the data and seperate it into one dataframes per patient. Raw acne severities are converted into % change in acne severity relative to average baseline. The average baseline is computed dynamically for each patient.\n",
    "2) Add a treatment history metadata column to each patient dataframe in the form ( (Treatment a1, Day 1), ..., (Treatment an, Day i)) where n is the index of a given treatment in the full history.\n",
    "3) Produce the distribution of % changes in acne severity over all histories and patients. Fit a kernel density estimate and display it for visual inspection.\n",
    "4) (Assumes a bimodal distribution); Optimize the KDE for local maxima and saddle point; find quantiles corresponding to local maxima and saddle points. Each quantile defines an acne severity change state. **In progress: dynamic decision of state number.*\n",
    "5) Assign an acne severity change state to each acne severity entry for all patient dataframes.       \n",
    "6) (Assumes a Dirichlet prior distribution for acne severity change probabilities) Calculate posterior distributions of acne severity change states for each consecutive treatment history. Plot each posterior as a striped heatmap for visual inspection. **In progress: dynamic decision of prior.*\n",
    "8) Calculate Kullback-Leibler Divergence between consecutive treatment history posteriors. Plot stepwise and cumulative KL Divergence for visual inspection. cumulative KL Divergence is interpreted as cumulative information gain from changing acne severity state distributions. \n",
    "9) Approximate concavity of cumulative KL Divergence vs. treatment history ordinal curve; use user provided percentile cutoff to define inflection point concavity threshold.\n",
    "10) Partion cumulative KL Divergence array between inflection points. Prevents regression model overfitting by checking if slope difference between adjacent inflection points exceeds user provided threshold. **In progress: further pruning of inflection points with AIC/BIC.*\n",
    "11) Fits linear regression model to each cumulative KL Divergence sublist; returns slope of each. Quantifies extent of change in acne severity state distribution pairwise. **In progress: Plotting treatment history vs. inflection point slope to reveal treatments responsible for greatest changes.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "2c2d4c8b-312d-4177-a37d-5caa2a0cf10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/miniconda3/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/lib/python3.12/site-packages (1.15.2)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.12/site-packages (2.2.4)\n",
      "Requirement already satisfied: seaborn in /opt/miniconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "#virtual environment \n",
    "! pip install matplotlib pandas scipy numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "e83b41d1-f7c2-4a97-948a-db0f878d1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle as rect\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import optimize\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from matplotlib.cm import viridis\n",
    "from scipy.stats import dirichlet \n",
    "from scipy.stats import beta\n",
    "from scipy.special import gammaln, psi\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "993ddf9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def seperate_patients(raw_data):\n",
    "    \"\"\"Function that separates the raw dataframe via the following:\n",
    "    1) Constructs an array tracking where the original date (2018-01-01) recurs.\n",
    "    2) Uses that array to split raw_data into seperate dataframes.\n",
    "    \"\"\"\n",
    "    #splitting data into different patients using leftmost column index (this is an assumption however)\n",
    "    seperatePatientsIndices = list(raw_data.index[raw_data[\"date\"] == \"2018-01-01\"])\n",
    "    seperatePatientsIndices.append(len(raw_data))\n",
    "\n",
    "    #checking to see if, after the same number of days in all dataframes, treatment of some sort was introduced\n",
    "    #verifies that on day 29 \n",
    "    #then adds the days each additional treatment was added after the fact (turns out they are all the same too)\n",
    "    allPatientsIntroDays = []\n",
    "    \n",
    "    #seperating single dataframe into list of dataframes for each patient\n",
    "    startIndex = 0\n",
    "    seperatePatientsDFs = []\n",
    "    for endIndex in seperatePatientsIndices[1:]:\n",
    "        seperatePatientsDFs.append(raw_data[startIndex:endIndex])\n",
    "        startIndex = endIndex\n",
    "     \n",
    "    for seperatePatientDF in seperatePatientsDFs:\n",
    "        treatmentIntroDays =  []\n",
    "        currentTreatment = seperatePatientDF[\"treatment\"].iloc[0]\n",
    "          \n",
    "        for lineIndex in range(1,len(seperatePatientDF)): \n",
    "            if seperatePatientDF[\"treatment\"].iloc[lineIndex] != currentTreatment:\n",
    "                treatmentIntroDays.append([currentTreatment, \"end day is\", lineIndex])\n",
    "                currentTreatment  = seperatePatientDF[\"treatment\"].iloc[lineIndex]\n",
    "                \n",
    "                \n",
    "        #remembering the end day of the last treatment and appending to the list\n",
    "        lastTreatment  = seperatePatientDF[\"treatment\"].iloc[len(seperatePatientDF)-1]\n",
    "        treatmentIntroDays.append( [lastTreatment, \"end day is\", len(seperatePatientDF) - 1 ] )\n",
    "         \n",
    "        allPatientsIntroDays.append(treatmentIntroDays)\n",
    "        \n",
    "    return seperatePatientsDFs, allPatientsIntroDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "02eddf59-990d-4fde-a9d7-b9b05272e594",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_history_metadata(seperatePatientsDFs, allPatientsIntroDays):\n",
    "\n",
    "    \"\"\"Function that adds a treatment history metadata column to each patient's dataframe by...\n",
    "    1) Loading each seperate patient's dataframe and compute the average baseline severity, normalizing acne severity scores. Then modifies the dataframe, called a severities dataframe.\n",
    "    2) For each, mapping each value for treatment to the a treatment history tuple. It is a tuple of the form ((days of treatment, ai), (days of treatment, ai+1),....(days of treatment, an))\n",
    "    where n is the row number of the current day of the particular treatment.\"\"\"\n",
    "    \n",
    "    \n",
    "    #initializing dict containing severties by day for a given treatment \n",
    "    severitiesDayTreatmentDict = {treatment: None for treatment in seperatePatientsDFs[0][\"treatment\"]}\n",
    "    \n",
    "    modifiedDFs = []\n",
    "    counter = 0 \n",
    "\n",
    "    for patient_DF, days_of_intro in zip(seperatePatientsDFs, allPatientsIntroDays):\n",
    "        #computing average baseline severity for each DF\n",
    "        average_bl = patient_DF[\"AcneSeverity\"].head(days_of_intro[0][2]).mean()\n",
    "        \n",
    "        #forming new dataframe from old one containing percent severity over baseline \n",
    "        modified_DF = patient_DF.copy()\n",
    "        \n",
    "        \n",
    "        modified_DF[\"AcneSeverity\"] = modified_DF[\"AcneSeverity\"].apply(lambda x: (x - average_bl)/average_bl)*100\n",
    "        modifiedDFs.append(modified_DF)\n",
    "        counter += 1\n",
    "    \n",
    "    metadata_DFs = []\n",
    "    #iterating over all dataframes and their respective treatment days of introduction\n",
    "    for severities_df, days_of_intro in zip (modifiedDFs, allPatientsIntroDays): \n",
    "        #for each dataframe and the corresponding set of days where a given treatment ends\n",
    "        \n",
    "        #adding an explicit day column to each severities dataframe to enable indexing with df.loc  \n",
    "        severities_df[\"day\"] = range(len(severities_df))\n",
    "    \n",
    "        #initializing the treatment history metadata column\n",
    "        severities_df[\"treatment_history\"] = None\n",
    "    \n",
    "        last_treat_index = 0 #keeps track of which number of the ordered list of treatments is currently being processed\n",
    "        treatment_days = {} #keeping track of how many days each particular treatment goes on for\n",
    "        treatment_history = [] #keeping track of the full history of which treatment occurs before the others and how long they last for\n",
    "    \n",
    "        #also keeping track of the last treatment\n",
    "        last_treatment_itself = None\n",
    "        \n",
    "        #iterating through rows of the dataframe\n",
    "        for row_index, row in severities_df.iterrows():\n",
    "            current_day = row[\"day\"]\n",
    "            current_treatment = row[\"treatment\"]\n",
    "                   \n",
    "            #also checking to see if the current treatment is entirely new or falls inside a different treatment block\n",
    "            if current_treatment != last_treatment_itself:\n",
    "                treatment_days[current_treatment] = 1  # First day of new treatment\n",
    "                treatment_history.append((current_treatment, 1))\n",
    "            else:\n",
    "                treatment_days[current_treatment] += 1\n",
    "                treatment_history[-1] = (current_treatment, treatment_days[current_treatment])\n",
    "                   \n",
    "           \n",
    "            #once history is built, the history is stored in the original dataframe as an entry in own column\n",
    "            severities_df.at[row_index, \"treatment_history\"] = list(treatment_history)\n",
    "    \n",
    "            last_treatment_itself = current_treatment  \n",
    "        #also modifying dataframes to remove baseline acne severities, as they've already been used\n",
    "        metadata_DFs.append(severities_df[days_of_intro[0][2]:])\n",
    "\n",
    "    return metadata_DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "f9b12034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_plot_severity_states(metadata_DFs):\n",
    "    \"\"\"Determines the quantile cutoff determining the quantiles corresponding to categorical acne severity states (low, medium, and high), by\n",
    "    1) Computing the KDE of the distribution of all normalized acne severity scores over all patients and treatment histories.\n",
    "    2) Using optimization to find the saddle point of the distribution and consolidating with the modes.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, figsize = (5, 5))\n",
    "    \n",
    "    \n",
    "    #collecting all severities, converting all to positive values, flattening as we go\n",
    "    all_severities = []\n",
    "    for df in metadata_DFs:\n",
    "        all_severities.extend(df[\"AcneSeverity\"] * -1)\n",
    "    \n",
    "    #check for normal character by plotting histogram\n",
    "    severities_histo = np.histogram(all_severities, density = True)\n",
    "    #axes.hist(all_severities, bins  = 30, density = True)\n",
    "    axes.set_title(\"Acne Severities Distribution (relative to baseline)\") \n",
    "    axes.set_xlabel(\"Normalized Severity Score\")\n",
    "    axes.set_ylabel(\"Density\")\n",
    "    \n",
    "    \n",
    "    #fitting a kernel density estimate to the data\n",
    "    \n",
    "    sns.kdeplot(all_severities, fill=True)\n",
    "    plt.title(\"KDE of Acne Severity Distribution\")\n",
    "    \n",
    "    #extracting the equation of the pdf and finding the local minimum in between the two modes\n",
    "    kde_pdf = sp.stats.gaussian_kde(all_severities)\n",
    "    \n",
    "    #using max and min of pdf to find saddle point in between 2 modes, sampling 1000 points\n",
    "    severity_grid = np.linspace(np.min(all_severities), np.max(all_severities), 1000)\n",
    "    \n",
    "    neg_kde = lambda x: -kde_pdf(x.reshape(1, -1))\n",
    "    \n",
    "    #finding the two main modes using optimization, with first 2 mode guesses at the .2 and .8 quantiles\n",
    "    guesses = np.percentile(all_severities, [20, 80]) \n",
    "    \n",
    "    modes = []\n",
    "    for guess in guesses:\n",
    "        better = optimize.minimize(neg_kde, np.array([guess])) \n",
    "        modes.append(better.x[0]) \n",
    "    \n",
    "    modes = np.array(modes)\n",
    "    \n",
    "    #finding the saddle point in between the two modes, using that as cutoff for the two patient states\n",
    "    \n",
    "    initial_guess = np.mean(modes) #average of the modes\n",
    "    bds = [(min(modes)+1, max(modes)-1)]  # This is a list of two tuples for each mode\n",
    "    \n",
    "    saddle_pt = optimize.minimize(kde_pdf, [initial_guess], bounds = bds) \n",
    "    state_ranges = [modes[0], saddle_pt.x[0], modes[1]]\n",
    "    state_names = [\"High Severity\", \"Medium Severity\", \"Low Severity\"]\n",
    "    \n",
    "    \n",
    "    #plotting modes and saddle point over the distribution\n",
    "    plt.scatter(modes[0], kde_pdf(modes[0]), color = \"red\", label = \"Lower Mode\")\n",
    "    plt.scatter(modes[1], kde_pdf(modes[1]), color = \"red\", label = \"Upper Mode\")\n",
    "    plt.scatter([saddle_pt.x[0]], kde_pdf([saddle_pt.x[0]]), color='green', label=\"Saddle Point\")\n",
    "    \n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return state_names, state_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "af4b582f-752c-46f0-8ce4-8c4192da188b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def assign_states_to_mdfs(metadata_DFs, state_names, state_ranges):\n",
    "    \"\"\"Function to construct and attach a second metadata column onto each patient's dataframe. The column contains\n",
    "    the acne severity categorical state corresponding to a given treatment history.\"\"\"\n",
    "    \n",
    "    for patient_df in metadata_DFs:\n",
    "        severity_states = np.digitize(-1 * patient_df[\"AcneSeverity\"], state_ranges, right = False)\n",
    "        severity_states = np.minimum(severity_states, 2)\n",
    "        patient_df[\"State\"] = [state_names[severity_state] for severity_state in severity_states] \n",
    "\n",
    "    return metadata_DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "a9f70894-26f3-4bbc-9f7c-8965c525ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_histograms(metadata_DFs):\n",
    "    \"\"\"This algorithm iterates over all patients' severity dataframes and, for each... \n",
    "\n",
    "    1) Counts all of the occcurences of each acne severity state throughout all patients, and assigns those counts \n",
    "    as a value to the treatment history key in the state counts dictionary.\n",
    "    2) Normalizes the counts into distributions before returning both. \n",
    "    \"\"\"\n",
    "    \n",
    "    all_state_counts = defaultdict(Counter)\n",
    "  \n",
    "    for i, patient_df in enumerate(metadata_DFs):\n",
    "        histories = patient_df[\"treatment_history\"].values\n",
    "        states = patient_df[\"State\"].values\n",
    " \n",
    "        for state_index in range(1, len(states)):\n",
    "            current_state = states[state_index] #state at position state_index in the patient's dataframe\n",
    "            current_history = histories[state_index] #metadata (treatment history) at position state_index  in the patient's dataframe\n",
    "            current_history_key = tuple((str(treatment), int(days)) for treatment, days in current_history)\n",
    "           \n",
    "            #recording the actual counts of severities, with the context of the prior treatment as the key\n",
    "            all_state_counts[current_history_key][current_state] += 1\n",
    "\n",
    "    #normalizing counts dictionaries into distributions\n",
    "    first_order_probabilities = {}\n",
    "    \n",
    "    for previous_treatment, state_counts in all_state_counts.items():\n",
    "        total_counts  = sum(state_counts.values())\n",
    "        probabilities_given_previous_state = {state: count/total_counts for state, count in state_counts.items()}\n",
    "        first_order_probabilities[previous_treatment] = probabilities_given_previous_state\n",
    "\n",
    "    return (all_state_counts, first_order_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "f891c9d8-479f-443f-a07d-880fd55b54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(first_order_probabilities):\n",
    "    \"\"\" This function plots a striped heatmap to inspect the distributions of normalized acne severity states for each treatment history.\n",
    "    It uses a viridis heatmap implementation from my other repository figuresAndViewers.\n",
    "    In lieu of using the actual treatment histories themselves as x labels, the x label is the index of the history in the sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_dim  = 100\n",
    "    y_dim = 200\n",
    "    \n",
    "    fig = plt.figure(figsize=(x_dim, y_dim))\n",
    "    matplotlib.rc('xtick', labelsize=14)\n",
    "    matplotlib.rc('ytick', labelsize=14)\n",
    "\n",
    "      \n",
    "    #plotting histograms of each context dependent model of acne treatment severity\n",
    "    bar_width = 0.3\n",
    "    spacing = 0.05 \n",
    "    \n",
    "    #sorting the distributions by the state name in reverse \n",
    "    \n",
    "    real_first_order_probabilities = dict(sorted(first_order_probabilities.items(), reverse = True))\n",
    "    #print(\"reals\", real_first_order_probabilities)\n",
    "    \n",
    "    mainPanelHeight = 15\n",
    "    mainPanelWidth = 20\n",
    "\n",
    "    legendPanelHeight = .25\n",
    "    legendPanelWidth = .5\n",
    "    \n",
    "    sidePanelHeight = 3\n",
    "    sidePanelWidth = .25\n",
    "    \n",
    "    \n",
    "    #setting up the panels and placing the proper positions\n",
    "    firstMainPanel = plt.axes([.05/x_dim,.375/y_dim, mainPanelWidth/x_dim, mainPanelHeight/\n",
    "    y_dim])\n",
    "    firstMainPanel.set_xlabel(\"Treatment History Index\")\n",
    "\n",
    "    #setting up the legend panel\n",
    "    legendRight = plt.axes([(1+mainPanelWidth)/x_dim, legendPanelHeight/y_dim, legendPanelWidth/x_dim, mainPanelHeight/y_dim])\n",
    "    #seting ticks of legend\n",
    "    legendRight.tick_params(bottom=False, labelbottom=False, left=True, labelleft=True, right=False, labelright=False, top=False, labeltop=False)\n",
    "    \n",
    "    legendRight.set_xlim(0,.1)\n",
    "    legendRight.set_ylim(0,20)\n",
    "    legendRight.set_yticks([0,20],['0','1'])\n",
    "    \n",
    " \n",
    "    #looping through to construct a heatmap for all distributions\n",
    "    entries = len(real_first_order_probabilities)\n",
    "    bar_width = 1 / entries\n",
    "    firstMainPanel.set_xlim(0, 1)\n",
    "    firstMainPanel.set_ylim(0, 1)\n",
    "\n",
    "    x_pos = 0\n",
    "    for history, raw_distribution in real_first_order_probabilities.items():\n",
    "        distribution = defaultdict(float, raw_distribution)\n",
    "        scaled_x_pos = x_pos * bar_width\n",
    "\n",
    "        high_fc = viridis(distribution[\"High Severity\"])[:3]\n",
    "        med_fc  = viridis(distribution[\"Medium Severity\"])[:3]\n",
    "        low_fc  = viridis(distribution[\"Low Severity\"])[:3]\n",
    "    \n",
    "\n",
    "        firstMainPanel.add_patch(rect([scaled_x_pos, 2/3], width=bar_width, height=1/3, facecolor=high_fc, edgecolor='black', linewidth=0.25))\n",
    "        firstMainPanel.add_patch(rect([scaled_x_pos, 1/3], width=bar_width, height=1/3, facecolor=med_fc, edgecolor='black', linewidth=0.25))\n",
    "        firstMainPanel.add_patch(rect([scaled_x_pos, 0],   width=bar_width, height=1/3, facecolor=low_fc, edgecolor='black', linewidth=0.25))\n",
    "\n",
    "        x_actual_spot = scaled_x_pos + bar_width / 2\n",
    "\n",
    "        x_pos += 1\n",
    "    num_rects = len(real_first_order_probabilities)\n",
    "    tick_positions = [i * bar_width + bar_width / 2 for i in range(num_rects)]\n",
    "\n",
    "    firstMainPanel.set_xticks(tick_positions)\n",
    "    firstMainPanel.set_xticklabels(['' for _ in tick_positions])\n",
    "    #firstMainPanel.set_yticks([1/6, 0.5, 5/6], [\"Low Severity\", \"Medium Severity\", \"High Severity\"], fontsize = 15)\n",
    "    firstMainPanel.set_yticks([1/6, 0.5, 5/6])\n",
    "    firstMainPanel.set_yticklabels([\"Low Severity\", \"Medium Severity\", \"High Severity\"], fontsize=15)\n",
    "        \n",
    "    #plotting viridis heatmap in the sidebar\n",
    "    #color map tuple pair linspaces, viridis values\n",
    "    vvLin1Red = np.linspace(68/255, 59/255, 5)\n",
    "    vvLin2Red = np.linspace(59/255, 33/255, 6)\n",
    "    vvLin3Red = np.linspace(33/255, 94/255, 6)\n",
    "    vvLin4Red = np.linspace(94/255, 253/255, 6)\n",
    "    \n",
    "    \n",
    "    vvLin1Green = np.linspace(1/255, 82/255, 5)\n",
    "    vvLin2Green = np.linspace(82/255, 145/255, 6)\n",
    "    vvLin3Green = np.linspace(145/255, 201/255, 6)\n",
    "    vvLin4Green = np.linspace(201/255, 231/255, 6)\n",
    "    \n",
    "    vvLin1Blue = np.linspace(84/255, 139/255, 5)\n",
    "    vvLin2Blue = np.linspace(139/255, 140/255, 6)\n",
    "    vvLin3Blue = np.linspace(140/255, 98/255, 6)\n",
    "    vvLin4Blue = np.linspace(98/255, 37/255, 6)\n",
    "    \n",
    "    \n",
    "    plLin4Red = np.linspace(245/255, 237/255, 5)\n",
    "    plLin3Red = np.linspace(190/255, 245/255, 6)\n",
    "    plLin2Red = np.linspace(87/255, 190/255, 6)\n",
    "    plLin1Red = np.linspace(15/255, 87/255, 6)\n",
    "    \n",
    "    plLin4Green = np.linspace(135/255,252/255, 5)\n",
    "    plLin3Green = np.linspace(48/255, 135/255, 6)\n",
    "    plLin2Green = np.linspace(0/255, 48/255, 6) \n",
    "    plLin1Green = np.linspace(0/255, 0/255, 6)\n",
    "    \n",
    "    plLin4Blue = np.linspace(48/255, 27/255, 5)\n",
    "    plLin3Blue = np.linspace(101/255, 48/255,  6)\n",
    "    plLin2Blue = np.linspace(151/255, 101/255, 6)\n",
    "    plLin1Blue = np.linspace(118/255, 151/255, 6)\n",
    "    \n",
    "    \n",
    "    #total linspaces for all tuple pairs, viridis values\n",
    "    vvListOfRedLins = list(vvLin1Red)+list(vvLin2Red)+list(vvLin3Red)+list(vvLin4Red)\n",
    "    vvListOfGreenLins = list(vvLin1Green)+list(vvLin2Green)+list(vvLin3Green)+list(vvLin4Green)\n",
    "    vvListOfBlueLins = list(vvLin1Blue)+list(vvLin2Blue)+list(vvLin3Blue)+list(vvLin4Blue)\n",
    "    \n",
    "    orderedVVRed = list(dict.fromkeys(vvListOfRedLins))\n",
    "    orderedVVGreen = list(dict.fromkeys(vvListOfGreenLins))\n",
    "    orderedVVBlue = list(dict.fromkeys(vvListOfBlueLins))\n",
    "    \n",
    "    \n",
    "    #viridis heatmaps into the legend panel\n",
    "    for index in range(0,20,1):\n",
    "    \tcolorPaletteVV = (orderedVVRed[index], orderedVVGreen[index], orderedVVBlue[index])\n",
    "    \tvvGradeRect = rect([0,index], .1, 8, facecolor=colorPaletteVV,edgecolor = 'black', linewidth = 0)\t\t\n",
    "    \tlegendRight.add_patch(vvGradeRect)\n",
    "\n",
    "    \n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "16b0c4a7-7d72-4a01-9289-bec85b3aee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Dirichlet(prior, all_transition_counts):\n",
    "    \"\"\"This function does the following: \n",
    "    Accepts a prior in order to construct a Bayesian model of each history's distribution as a Dirichlet distribution with a multinomial \n",
    "    likelihood. It does so by the following methods. \n",
    "    1)  Iterates through each set of counts for each history and adds them to each parameter in order in the prior, then uses these to\n",
    "    build the posterior Dirichlet distribution.\"\"\"\n",
    "\n",
    "    categories = ['Low Severity', 'Medium Severity', 'High Severity']\n",
    "    prior_dict = {'Low Severity': prior[0], \"Medium Severity\": prior[1], \"High Severity\": prior[2]}\n",
    "    \n",
    "    history_and_posteriors = {}\n",
    "\n",
    "    \n",
    "    for history, count_dict in all_transition_counts.items():\n",
    "        \n",
    "        counts = [count_dict.get(cat, 0) for cat in categories] #pulling counts from prior dict and counts dict\n",
    "        prior = [prior_dict[cat] for cat in categories] \n",
    "\n",
    "        #updating parameters for posterior distribution\n",
    "        posterior_params = np.array(counts) + np.array(prior)\n",
    "            \n",
    "        \n",
    "        #saving posterior params to the dictionary\n",
    "        history_and_posteriors[history] = posterior_params\n",
    "        \n",
    "    return history_and_posteriors, categories   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "46c178be-9f94-45ab-b455-6011f2002cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dirichlet_marginal_cis(alphas, confidence_level = .95):\n",
    "    \"\"\"Function that is wrapped by the below function. Calculates the upper and lower quantiles supplied by confidence interval\n",
    "    for each marginal beta distribution of a given dirichlet. Returns each confidence interval indexed by the respective alpha.\"\"\"\n",
    "    confidence_intervals = {}\n",
    "    top_density = (1 - confidence_level)/2\n",
    "    bottom_density = 1 - top_density\n",
    "    total_alpha = np.sum(alphas)\n",
    "    \n",
    "\n",
    "    for index, alpha in enumerate(alphas):\n",
    "        other_alpha_sum = total_alpha - alpha\n",
    "        lower_bound = beta.ppf(bottom_density, alpha, other_alpha_sum)\n",
    "        upper_bound = beta.ppf(top_density, alpha, other_alpha_sum)\n",
    "        confidence_intervals[index] = (lower_bound, upper_bound)\n",
    "\n",
    "    return confidence_intervals\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "c5d6430c-3d34-4643-8e74-0e45a68924cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Dirichlets_credible_interals(histories_and_dirichlets, ordered_categories, confidence_level = .95):\n",
    "    \"\"\"This function finds the 95% credible intervals for each component of the Dirichlet distribution for all treatment histories.\n",
    "    It ensures that a stacked plot is made.\"\"\"\n",
    "    \n",
    "    #checking to see if no categories are supplied; just uses indices of each state to name categories in that case\n",
    "    if ordered_categories is None:\n",
    "        ordered_categories = [f\"State {i}\" for i in range(len(dirichlet_posteriors[0]))]\n",
    "    \n",
    "    fig, ax = plt.subplots(len(ordered_categories), 1, figsize=(len(ordered_categories) * 3.5, 7), sharex = True)\n",
    "    ax[-1].set_xlabel(\"History Index\")\n",
    "\n",
    "    jitter_spacing = .5\n",
    "    left_end = 0 \n",
    "    for history, alphas in histories_and_dirichlets.items():\n",
    "        \n",
    "        these_confidence_intervals = find_dirichlet_marginal_cis(alphas)\n",
    "        for subplot_index, ordered_confidence_interval in these_confidence_intervals.items():\n",
    "            #ax[subplot_index].set_xticks(np.arange(len(histories_and_dirichlets)))\n",
    "            #ax[subplot_index].set_xticklabels(list(histories_and_dirichlets.keys()))\n",
    "            x = left_end + (subplot_index - len(these_confidence_intervals)/2) * jitter_spacing  # adding some x offset for the error bars\n",
    "            center = (ordered_confidence_interval[1] + ordered_confidence_interval[0])/2\n",
    "            width = ordered_confidence_interval[0] - ordered_confidence_interval[1]\n",
    "            \n",
    "            ax[len(ordered_categories) - subplot_index - 1].errorbar(x, center, yerr = width/2, fmt='o', color='C0', capsize=5)\n",
    "            \n",
    "        left_end += 1\n",
    "            \n",
    "    for i, name in enumerate(reversed(ordered_categories)):\n",
    "        ax[i].set_ylabel(name)\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "f30a3960-860a-4936-9901-51857787cc69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_KL_Divergence_vectorized(histories_and_posteriors):\n",
    "    \"\"\"This vectorized function calculates the Kullback-Leibler divergence between adjacent \n",
    "    3 dimensional Dirichlet distributions, each of which is indexed by its alpha parameter.\n",
    "    It does this for a full array, computing KL Divergence for each term alpha[i] and alpha[i+1]. So, it requires \n",
    "    you to supply two arrays of parameters, but really, just the same one read forwards and backwards.\n",
    "    \"\"\"\n",
    "    history_labels = list(histories_and_posteriors.keys())\n",
    "    x_vals = np.arange(1, len(history_labels))\n",
    "\n",
    "    alphas = np.array(list(histories_and_posteriors.values()))\n",
    "    alphas_backward = alphas[:-1]\n",
    "    alphas_forward = alphas[1:]\n",
    "\n",
    "    #ensuring alphas are non-0 up front\n",
    "    sum_forward = np.sum(alphas_forward, axis=1)\n",
    "    sum_backward = np.sum(alphas_backward, axis=1)\n",
    "\n",
    "    first_term = gammaln(sum_forward) - gammaln(sum_backward)\n",
    "    second_term = np.sum(gammaln(alphas_backward) - gammaln(alphas_forward), axis=1)\n",
    "    third_term = np.sum(\n",
    "        (alphas_forward - alphas_backward) * \n",
    "        (psi(alphas_forward) - psi(sum_forward)[:, None]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    kl_div = first_term + second_term + third_term\n",
    "    cumulative_kl = np.cumsum(kl_div)\n",
    "    cumulative_kl = np.clip(cumulative_kl, 1e-10, None)  # Avoid log(0)\n",
    "\n",
    "    return x_vals, np.log(cumulative_kl), cumulative_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "847a51f9-b0c2-4b11-a5fd-0636d9e308ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood(points, predictions):\n",
    "    \"\"\"This function is wrapped by fit_piecewise_regression. It uses maximum-likelihood estimation\n",
    "    to estimate the MLE of variance in the residuals for a linear regression model. \n",
    "    It then uses the sample size of the points from the model to find the log likelihood of \n",
    "    the actual points given the model fit to it.\"\"\"\n",
    "\n",
    "    residuals = np.array(points - predictions)\n",
    "    mle_variance_residuals = (1/len(points)) * np.sum((residuals ** 2))\n",
    "    log_likelihood = (-len(points)/2) * (np.log10(2*np.pi) + np.log(mle_variance_residuals) + 1) \n",
    "\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "c898ff44-216c-4099-88c4-f399172077bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def split_for_piecewise_regression(xs, cumulative_kls, percentile_cutoff = 80, split_index = None, gaussian_sigma = 1, slope_threshold = 0.2,\n",
    "                            min_segment_length = 3):\n",
    "    \"\"\"This algorithm uses a brute force method to find the inflection points to fit piecewise regression models to the data.\n",
    "    But first...\n",
    "    It dynamically chooses the right number of inflection points to fit each model to via the following...\n",
    "    1) It smoothes the cumulative KL divergence array with a Gaussian filter (by default is standard normal). \n",
    "    2) It computes pairwise slopes between adjacent cumulative KL values (with each x interval = 1/number of steps in treatment history)\n",
    "    ^check that later\n",
    "    3) Computes differences between adjacent slopes, returning a 2nd derivative approximation for the entire array\n",
    "    4) (Currently commented out) Plots the distribution of the magnitudes for inspection.\n",
    "    5) A cutoff for inflection points is chosen as a percentile of the 2nd derivative magntitdes (default is 90th). \n",
    "\n",
    "    Then, it iterates through the found inflection points, dividing them into subarrays of consecutive points. Once it does this, it does \n",
    "    one final check to see if each subarray should be further divided. It does this by iterating through each array, comparing the difference in \n",
    "    adjacent slopes pairwise, checking their difference against the provided slope threshold parameter. It then makes a list of lists of \n",
    "    inflection points where the regression models should start and end.\n",
    "\n",
    "    It then uses the result of this to further divide the points into subarrays that a regression model can be fit to.\n",
    "    At last, it checks the length of each subarray against the minimum segment length, and discards ones that are too small.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    smoothed_kls = gaussian_filter1d(cumulative_kls, sigma = gaussian_sigma)\n",
    "    slopes = np.diff(smoothed_kls)\n",
    "    second_derivatives_magnitudes = np.abs(np.diff(slopes))\n",
    "    threshold = np.percentile(second_derivatives_magnitudes, percentile_cutoff)\n",
    "    inflection_points = np.where(second_derivatives_magnitudes > threshold)[0] + 1\n",
    "\n",
    "    differences = np.diff(inflection_points)\n",
    "    non_consecutive_indices = np.where(differences != 1)[0] + 1 \n",
    "    separated_inflection_points = np.array_split(inflection_points, non_consecutive_indices)\n",
    "    \n",
    "    #adding the end of the array to the non_consecutive indices for easier splitting later\n",
    "    separated_inflection_points[-1] = np.append(separated_inflection_points[len(separated_inflection_points)-1], len(smoothed_kls)-1)    \n",
    "    all_breaks = []\n",
    "\n",
    "    \n",
    "    for consecutives_array in separated_inflection_points:\n",
    "        index_consecutives_array = 0\n",
    "        last_inflection_pt = 0 \n",
    "        last_slope = 0\n",
    "\n",
    "        consecutives_breaks = []\n",
    "        \n",
    "        for an_inflection_pt in consecutives_array:\n",
    "            if an_inflection_pt >= len(slopes):\n",
    "                continue\n",
    "            slope_current = slopes[an_inflection_pt]\n",
    "\n",
    "            slope_difference = np.abs(slope_current - last_slope)\n",
    "            \n",
    "            \n",
    "            if slope_difference > slope_threshold:\n",
    "                consecutives_breaks.append(an_inflection_pt)\n",
    "            \n",
    "            last_slope = slope_current\n",
    "            last_inflection_pt = an_inflection_pt\n",
    "\n",
    "        all_breaks.append((consecutives_breaks, index_consecutives_array))\n",
    "\n",
    "        index_consecutives_array += 1 \n",
    "   \n",
    "\n",
    "    fixed_consecutives = []\n",
    "    for breaks, index in all_breaks:\n",
    "        \n",
    "        \n",
    "        if len(breaks) != 0:\n",
    "            consecutives_array_to_split = separated_inflection_points[index]\n",
    "            last_breaking_index = breaks[0] - 1\n",
    "            for breaking_index in breaks:\n",
    "                \n",
    "                broken_consecutives_array = consecutives_array_to_split[last_breaking_index: breaking_index+1]\n",
    "                fixed_consecutives.append(broken_consecutives_array)\n",
    "                last_breaking_index = breaking_index\n",
    "    \n",
    "    #clipping the appropriate array in the inflection point array of arrays \n",
    "    clipped_inflection_points = []\n",
    "    last_consecutive_break = None\n",
    "\n",
    "    # Find the last break if it exists\n",
    "    if fixed_consecutives:\n",
    "        last_consecutive_break = fixed_consecutives[-1][-1]\n",
    "\n",
    "    # Search for that break in the separated inflection points\n",
    "    where_clipping = None\n",
    "    for i, again_consecutives_array in enumerate(separated_inflection_points):\n",
    "        if last_consecutive_break in again_consecutives_array:\n",
    "            idx = np.where(again_consecutives_array == last_consecutive_break)[0][0]\n",
    "            clipped_inflection_points.append(again_consecutives_array[idx + 1:])\n",
    "            where_clipping = i\n",
    "            break  # stop after finding the first match\n",
    "\n",
    "    # Combine final splits\n",
    "    if where_clipping is not None:\n",
    "        full_splits = fixed_consecutives + clipped_inflection_points + separated_inflection_points[where_clipping+1:]\n",
    "    else:\n",
    "        full_splits = fixed_consecutives\n",
    "\n",
    "    return full_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "03e8ea10-e0b9-4c01-bb5a-fd31bece6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_piecewise_regression_segments(curve, splits, ax, color = \"blue\", linewidth = 2):\n",
    "    \"\"\"This function is wrapped by piecewise_regression_and_plot below. It plots the actual line segments piecewise\n",
    "    over a rendered cumulative KL Divergence plot. \"\"\"\n",
    "\n",
    "    if splits is None:\n",
    "        print(\"No splits to plot!\")\n",
    "        return\n",
    "    \n",
    "    for split in splits:\n",
    "        split_start, split_end = split[0], split[len(split)-1]\n",
    "        y_segment = curve[split_start: split_end + 1] \n",
    "        x_segment = np.arange(split_start, split_end+1).reshape(-1, 1)\n",
    "\n",
    "        regression_piece = LinearRegression().fit(x_segment, y_segment.reshape(-1, 1))\n",
    "        y_pred = regression_piece.predict(x_segment)\n",
    "        ax.plot(x_segment.flatten(), y_pred.flatten(), color=color, linewidth=linewidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "c6ade13b-c258-449b-a2bb-d44ad151d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_KL_Divergence_plot(x_vals, kl_divergences, cumulative_kl, splits):\n",
    "    \"\"\"This function plots the Kullback-Leibler Divergence between each consecutive Dirichlet posterior distribution\n",
    "    as a line plot. This is a wrapper of calculate_KL_Divergence_vectorized.\n",
    "    It also plots the cumulative KL Divergence over the KL divergence between individual distributions.\n",
    "    It ends up plotting the linear regression models over the plot as well. \"\"\"\n",
    "   \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    linear_models = plot_piecewise_regression_segments(cumulative_kl, splits, ax, linewidth = 1)\n",
    "    ax.semilogy(x_vals[0:], kl_divergences[0:], marker='o', label='KL Divergence')\n",
    "    ax.semilogy(x_vals[0:], cumulative_kl[0:], marker='o', linestyle='--', color='orange', label='Cumulative KL Divergence', alpha = .5)\n",
    "\n",
    "    ax.set_title('KL Divergence and Cumulative Information Gain (Log Scale)')\n",
    "    ax.set_xlabel('Treatment History Index')\n",
    "    ax.set_ylabel('KL Divergence (log scale)')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    return x_vals, np.log(cumulative_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "47273c56-e2d2-44a9-a83a-8621ecfc7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving Acne04 dataset from Kaggle source (local download on machine)\n",
    "raw_data = pd.read_csv(\"archive (2)/sim_acne.csv\")\n",
    "\n",
    "#actual implementation\n",
    "this_seperate_DFs, this_intro_days = seperate_patients(raw_data)\n",
    "this_md_DFs = add_history_metadata(this_seperate_DFs, this_intro_days)\n",
    "these_states, these_ranges = find_and_plot_severity_states(this_md_DFs)\n",
    "\n",
    "\n",
    "\n",
    "these_assigned_md_DFs = assign_states_to_mdfs(this_md_DFs, these_states, these_ranges)\n",
    "built_histograms, raw_probabilities = build_histograms(these_assigned_md_DFs)\n",
    "\n",
    "plotted_histogram = plot_histograms(raw_probabilities)\n",
    "\n",
    "uninformative_prior = [1,1,1] #with a1 corresponding to low severity, a2 corresponding to medium, and a3 corresponding to high\n",
    "\n",
    "these_Dirichlets, these_categories = build_Dirichlet(uninformative_prior, built_histograms)\n",
    "\n",
    "dirichlet_credible_intervals = plot_Dirichlets_credible_interals(these_Dirichlets, these_categories)\n",
    "\n",
    "these_xs, this_log_cum_kls, this_kls = calculate_KL_Divergence_vectorized(these_Dirichlets)\n",
    "splits = split_for_piecewise_regression(these_xs, this_log_cum_kls)\n",
    "final_plot = make_KL_Divergence_plot(these_xs, this_log_cum_kls, this_kls, splits)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "27484e4b-a5be-492b-bd50-a9d4c87ffc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_piecewise_regression_and_plot_unused(points, splits):\n",
    "    \"\"\"This function actually splits a given array (here, cumulative KL divergence) into seperate regression models,\n",
    "    using splits to partition the array and then fit a linear regression model to each one.\n",
    "    It then calls plot_piecewise_regression_segments to plot the segments over the cumulative KL divergence Curve. Unused in this version.\"\"\"\n",
    "    split_points_and_indices = [(points[split[0]:split[1]+1], split) for split in splits]\n",
    "    linear_model = LinearRegression()\n",
    "    consecutive_models = [LinearRegression().fit(np.arange(len(one_split_points[0])).reshape(-1, 1),one_split_points[0].reshape(-1, 1))for one_split_points in split_points_and_indices]\n",
    "\n",
    "    slopes = [which_model.coef_[0] for which_model in consecutive_models]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "17f992a3-b068-4e7e-9bb4-0ebdf84ca2de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def old_build_histograms(metadata_DFs):\n",
    "    \"\"\"Unused in this version. Will condition distributions on previous state and treatment history. Stay tuned... \n",
    "    \n",
    "    \n",
    "    1) The algorithm iterates over all patient's severity dataframe and, for each... \n",
    "    \n",
    "    Uses the metadata column to build an actual sequence of treatments for each patient along with the state at each. \n",
    "    It does this by pulling the treatment history column for each dataset as well as the states, both as lists. Then, it uses a default dict to\n",
    "    count the occurences of transition from state to state for the current treatment and the one before it. In other words, a dictionary\n",
    "    of dictionaries is returned, where the keys are the treatments in order of occurence, and the values are dictionaries of counts \n",
    "    for each state in the treatment order. \n",
    "    \n",
    "    This amounts to a context-dependent conditional model of acne treatement severity. \n",
    "    \n",
    "    A context-dependent First Order Markov Model is also built in the second loop, using instead a tuple of the previous state and preivous \n",
    "    treatment as a key in the default dictionary. \n",
    "    \n",
    "    2) Then, histograms of the context-dependent conditional model of acne treatment severity are plotted. After this, a 3 State visual\n",
    "    Markov Model is generated for the 1st order Markov Model. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    all_state_counts = defaultdict(Counter)\n",
    "    #all_transition_counts_second_order = defaultdict(Counter)\n",
    "    \n",
    "    for i, patient_df in enumerate(metadata_DFs):\n",
    "        histories = patient_df[\"treatment_history\"].values\n",
    "        states = patient_df[\"State\"].values\n",
    "\n",
    "        \n",
    "        \n",
    "        for state_index in range(1, len(states)):\n",
    "            current_state = states[state_index] #state at position state_index in the patient's dataframe\n",
    "            #previous_history = histories[state_index - 1] #metadata (treatment history) at position state_index - 1 in the patient's dataframe\n",
    "            current_history = histories[state_index] #metadata (treatment history) at position state_index  in the patient's dataframe\n",
    "            current_history_key = tuple((str(treatment), int(days)) for treatment, days in current_history)\n",
    "           \n",
    "            \n",
    "            #previous_treatment = previous_history[-1][0] #previous treatment right before position state_index\n",
    "            #current_treatment = current_history[-1][0] #current treatment at state_index\n",
    "    \n",
    "            #recording the actual counts of severities, with the context of the prior treatment as the key\n",
    "            all_state_counts[current_history_key][current_state] += 1\n",
    "            \n",
    "        \n",
    "        #doing the same for the First order Markov Chain (UNUSED)\n",
    "        # for state_index in range(2, len(states)):\n",
    "        #     last_state = states[state_index - 1] \n",
    "        #     this_current_state = states[state_index] #state at position state_index in the patient's dataframe\n",
    "        #     this_previous_history = histories[state_index - 1] #metadata (treatment history) at position state_index - 1 in the patient's dataframe\n",
    "            \n",
    "        #     this_current_history = histories[state_index] #metadata (treatment history) at position state_index  in the patient's dataframe\n",
    "        #     this_previous_treatment = this_previous_history[-1][0] #previous treatment right before position state_index\n",
    "        #     this_current_treatment = this_current_history[-1][0] #current treatment at state_index\n",
    "    \n",
    "        #     this_key = (this_previous_treatment, last_state)\n",
    "        #     #recording the actual counts of severities, with the context of the prior treatment as the key\n",
    "        #     all_transition_counts_second_order[this_key][current_state] += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "    #normalizing counts dictionaries into distributions\n",
    "    first_order_probabilities = {}\n",
    "    second_order_probabilities = {}\n",
    "    \n",
    "    for previous_treatment, state_counts in all_state_counts.items():\n",
    "        total_counts  = sum(state_counts.values())\n",
    "        probabilities_given_previous_state = {state: count/total_counts for state, count in state_counts.items()}\n",
    "        first_order_probabilities[previous_treatment] = probabilities_given_previous_state\n",
    "        \n",
    "        \n",
    "    # for (previous_treatment, previous_state), state_counts in all_transition_counts_second_order.items(): (UNUSED)\n",
    "    #     total_counts  = sum(state_counts.values())\n",
    "    #     probabilities_given_previous_state = {state: count/total_counts for state, count in state_counts.items()}\n",
    "    #     second_order_probabilities[(previous_treatment, previous_state)] = probabilities_given_previous_state\n",
    "\n",
    "    print(all_state_counts)\n",
    "\n",
    "    return (all_state_counts, first_order_probabilities) #, (all_transition_counts_second_order, second_order_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04df51-3bbd-4c16-913c-7ae1312f15f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
