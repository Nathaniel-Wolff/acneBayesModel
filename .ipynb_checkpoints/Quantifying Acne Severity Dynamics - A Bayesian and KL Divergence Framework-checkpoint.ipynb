{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb829b9-f2ed-458a-b8fd-0cc0b22ab3d7",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Quantifying Acne Severity Dynamics - A Bayesian and KL Divergence Framework</h1>\n",
    "<p style=\"text-align:center;\">By Nathaniel Wolff.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa384b",
   "metadata": {},
   "source": [
    "# Analysis Structure and Results\n",
    "\n",
    "Dataset is found here: https://www.kaggle.com/datasets/manuelhettich/acne04. Consists of n = 10 synthetic patients, \n",
    "with acne severity recorded over 112 distinct days of treatment (each indexing a distinct treatment history).\n",
    "\n",
    "\n",
    "### Analysis Steps\n",
    "#### 1) Data Parsing -\n",
    "Base dataframe is seperated by patient. Raw acne severities are normalized as % change relative to patient average baseline intensity.\n",
    "\n",
    "#### 2) Treatment History Metadata Addition - \n",
    "A treatment history metadata column is added to each patient dataframe. Histories are of the form ( (Treatment $a_{1}$, Day 1),...(Treatment $a_{n}$, Day i)), where n is the index of a given treatment in the full history.\n",
    "\n",
    "#### 3) Kernel Density Estimation of normalized acne severity change -\n",
    "Over all patient dataframes, the distribution of normalized acne severity over all histories and patients is obtained, and a Kernel Density Estimate is fit.\n",
    "\n",
    "#### 4) Optimization of KDE and Discretization of Acne Severity Change into 3 states -\n",
    "The KDE is optimized for local maxima and saddle points; corresponding quantiles define an acne severity change states.  \n",
    "*In progress: dynamic decision of state number.*\n",
    "\n",
    "#### 5) Validation of Discretization via Quantile Bootstrapping -\n",
    "Before binning normalized acne severities into change states, the robustness of binning is assessed via bootstrapping of each quantile. \n",
    "\n",
    "#### 6) Calculation of Acne Severity Change State Distribution Series over treatment sequence - \n",
    "With an uninformative Dirhclet prior and multinomial likelihood,  posterior distributions of acne severity change states are calculated for each treatment history. \n",
    "\n",
    "#### 7) Calculation of information cost between consecutive distributions - \n",
    "Kullback-Leibler divergence is calculated between consecutive treatment history posteriors. Cumulative KL divergence represents the information cost of not updating distribution from day to day, a descriptor of distance between distributions. \n",
    "\n",
    "#### 8) Determination of Diminishing Returns: Fitting of Univariate Spline to Cumulative KL divergence curve - \n",
    "A univariate spline curve is fit to the cumulative KL Divergence of the entire treatment series. The curve's first derivative is evaluated against a threshold (\"p value\"), returning sections of non-diminishing returns in treatment. \n",
    "\n",
    "#### 9) Fitting of Linear Model to Non-Diminishing Treatment Returns' Cumulative KL Divergence\n",
    "A linear regression model is fit to each non-diminishing return section. Histories are plotted on the x-axis. The systems biology/biochemical implications of these sections are used to determine the form of the predictive model below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "2c2d4c8b-312d-4177-a37d-5caa2a0cf10c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/miniconda3/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/lib/python3.12/site-packages (1.15.2)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.12/site-packages (2.2.4)\n",
      "Requirement already satisfied: seaborn in /opt/miniconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: statsmodels in /opt/miniconda3/lib/python3.12/site-packages (0.14.5)\n",
      "Requirement already satisfied: filterpy in /opt/miniconda3/lib/python3.12/site-packages (1.4.5)\n",
      "Requirement already satisfied: numdifftools in /opt/miniconda3/lib/python3.12/site-packages (0.9.41)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/miniconda3/lib/python3.12/site-packages (from statsmodels) (1.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "#virtual environment \n",
    "! pip install matplotlib pandas scipy numpy seaborn scikit-learn statsmodels filterpy numdifftools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "e83b41d1-f7c2-4a97-948a-db0f878d1694",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import matplotlib\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle as rect\n",
    "from itertools import permutations\n",
    "from colorsys import rgb_to_hls, hls_to_rgb\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import optimize\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from collections import defaultdict, Counter\n",
    "from matplotlib.cm import viridis\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import dirichlet \n",
    "from scipy.stats import beta\n",
    "from scipy.special import gammaln, psi\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import json\n",
    "import filterpy as fp\n",
    "import numdifftools as nd\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "993ddf9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def seperate_patients(raw_data):\n",
    "    \"\"\"Function that separates the raw dataframe via the following:\n",
    "    1) Constructs an array tracking where the original date (2018-01-01) recurs.\n",
    "    2) Uses that array to split raw_data into seperate dataframes.\n",
    "    \"\"\"\n",
    "    #splitting data into different patients using leftmost column index (this is an assumption however)\n",
    "    seperatePatientsIndices = list(raw_data.index[raw_data[\"date\"] == \"2018-01-01\"])\n",
    "    seperatePatientsIndices.append(len(raw_data))\n",
    "\n",
    "    #checking to see if, after the same number of days in all dataframes, treatment of some sort was introduced\n",
    "    #verifies that on day 29 \n",
    "    #then adds the days each additional treatment was added after the fact (turns out they are all the same too)\n",
    "    allPatientsIntroDays = []\n",
    "    \n",
    "    #seperating single dataframe into list of dataframes for each patient\n",
    "    startIndex = 0\n",
    "    seperatePatientsDFs = []\n",
    "    for endIndex in seperatePatientsIndices[1:]:\n",
    "        seperatePatientsDFs.append(raw_data[startIndex:endIndex])\n",
    "        startIndex = endIndex\n",
    "     \n",
    "    for seperatePatientDF in seperatePatientsDFs:\n",
    "        treatmentIntroDays =  []\n",
    "        currentTreatment = seperatePatientDF[\"treatment\"].iloc[0]\n",
    "          \n",
    "        for lineIndex in range(1,len(seperatePatientDF)): \n",
    "            if seperatePatientDF[\"treatment\"].iloc[lineIndex] != currentTreatment:\n",
    "                treatmentIntroDays.append([currentTreatment, \"end day is\", lineIndex])\n",
    "                currentTreatment  = seperatePatientDF[\"treatment\"].iloc[lineIndex]\n",
    "                \n",
    "                \n",
    "        #remembering the end day of the last treatment and appending to the list\n",
    "        lastTreatment  = seperatePatientDF[\"treatment\"].iloc[len(seperatePatientDF)-1]\n",
    "        treatmentIntroDays.append( [lastTreatment, \"end day is\", len(seperatePatientDF) - 1 ] )\n",
    "         \n",
    "        allPatientsIntroDays.append(treatmentIntroDays)\n",
    "        \n",
    "    return seperatePatientsDFs, allPatientsIntroDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "6dcf792a-8822-4a2c-820e-73f2f3c77240",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_generator_starting_colors(number_treatments, base_colormap = \"viridis\"):\n",
    "    \"\"\"A helper function for display_plots_of_dataset below. Generates n RGB tuples on a linear scale, given the number of treatments.\n",
    "    Each of these is used as a starting point for a regular map of treatments by day and a blended heatmap of treatments by day, with\n",
    "    lower lightness for each consecutive day of the same treatment, blended with the last color corresponding to the days of the\n",
    "    previous treatments.\"\"\"\n",
    "\n",
    "    cmap = plt.get_cmap(base_colormap)\n",
    "    default_colors = [cmap(i / (number_treatments - 1))[:3] for i in range(number_treatments)]\n",
    "    return default_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "8b4d89ae-b7d1-4874-bf71-17a4dfa44207",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def blend_old_and_new_color(old_color, new_color, alpha):\n",
    "    \"\"\"Another function used by display_plots_of_dataset. Simply blends 2 colors with a given alpha value.\"\"\"\n",
    "     # normalize both old and new colors into decimals\n",
    "    o = np.array(old_color)\n",
    "    n = np.array(new_color)\n",
    "\n",
    "    \n",
    "    o_h, o_l, o_s = rgb_to_hls(*o)\n",
    "    n_h, n_l, n_s = rgb_to_hls(*n)\n",
    "    \n",
    "    # circular interpolation of hue for more effective blending\n",
    "    hue_diff = ((n_h - o_h + 0.5) % 1.0) - 0.5\n",
    "    blended_h = (o_h + alpha * hue_diff) % 1.0\n",
    "    blended_l = (1 - alpha) * o_l + alpha * n_l\n",
    "    blended_s = (1 - alpha) * o_s + alpha * n_s\n",
    "    \n",
    "    blended_rgb = np.array(hls_to_rgb(blended_h, blended_l, blended_s))\n",
    "    return tuple(blended_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "5b6e7ddf-15e0-43e9-8294-fdb9c952eb09",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def display_plots_of_dataset(separate_dfs, patient_intro_days, alpha = .3):\n",
    "    \"\"\"This function plots the raw data for inspection.\n",
    "    Each patient corresponds to a series of rectangles, with face color corresponding to treatment.\n",
    "    The facecolor changes brightness based on the number of days of a consecutive treatment are applied,\n",
    "    setting up potential for conditional dependency analysis between treatment history distributions later\n",
    "    on. It uses a colorsys mapping between a given starting set of RGB tuples, one for each type of \n",
    "    treatment.\"\"\"\n",
    " \n",
    "    x_dim  = 200\n",
    "    y_dim = 200\n",
    "    \n",
    "    data_fig = plt.figure(figsize=(x_dim, y_dim))\n",
    "    matplotlib.rc('xtick', labelsize=14)\n",
    "    matplotlib.rc('ytick', labelsize=14)\n",
    "    \n",
    "    mainPanelHeight = 3.75\n",
    "    mainPanelWidth = 5\n",
    "\n",
    "    otherMainPanelHeight = 3.75\n",
    "    otherMainPanelWidth = 5\n",
    "\n",
    "    legendPanelHeight = 3\n",
    "    legendPanelWidth = .5\n",
    "    \n",
    "    sidePanelHeight = 3\n",
    "    \n",
    "    sidePanelWidth = .25\n",
    "    \n",
    "    \n",
    "    #setting up the panels and placing the proper positions\n",
    "    firstMainPanel = plt.axes([.05/x_dim,.375/y_dim, mainPanelWidth/x_dim, mainPanelHeight/\n",
    "    y_dim])\n",
    "    firstMainPanel.set_xlabel(\"Treatment Day\")\n",
    "    firstMainPanel.set_ylabel(\"Patient ID\")\n",
    "    firstMainPanel.set_title(\"Dataset Overview\")\n",
    "\n",
    "    otherMainPanel = plt.axes([.05/x_dim,(1.25+mainPanelHeight)/y_dim, otherMainPanelWidth/x_dim, otherMainPanelHeight/\n",
    "    y_dim])\n",
    "    otherMainPanel.set_xlabel(\"Treatment Day\")\n",
    "    otherMainPanel.set_ylabel(\"Patient ID\")\n",
    "    otherMainPanel.set_title(\"Dataset Overview (Blended)\")\n",
    "\n",
    "    main_bottom = 0.375 / y_dim\n",
    "    main_height = mainPanelHeight / y_dim\n",
    "    main_top = main_bottom + main_height\n",
    "\n",
    "    other_bottom = (1.25 + mainPanelHeight) / y_dim\n",
    "\n",
    "    legend_bottom = (main_top + other_bottom) / 2\n",
    "\n",
    "    #setting up the legend panel\n",
    "    legendRight = plt.axes([(1.5+otherMainPanelWidth)/x_dim,legend_bottom-(.5*legendPanelHeight)/y_dim, legendPanelWidth/x_dim, legendPanelHeight/y_dim])\n",
    "    #seting ticks of legend\n",
    "    legendRight.set_title(\"Treatments (Base Color)\")\n",
    "    legendRight.tick_params(bottom=False, labelbottom=False, left=True, labelleft=True, right=False, labelright=False, top=False, labeltop=False)\n",
    "    \n",
    "    \n",
    "\n",
    "    all_patient_IDS = set()\n",
    "    \n",
    "    starting_colors_dict = defaultdict(tuple)\n",
    "    #getting the set of all treatments\n",
    "    all_treatments_all_patients = set() #mental note: trying to make the mapping work for default colors for each treatment with default dict\n",
    "    for separated_dataframe in separate_dfs: \n",
    "        patient_ID  = set(separated_dataframe[\"patient_id\"].tolist())\n",
    "        \n",
    "        all_patient_IDS.update(patient_ID)\n",
    "        treatments_series_set = set(separated_dataframe[\"treatment\"].tolist())\n",
    "        all_treatments_all_patients.update(treatments_series_set)\n",
    "    #finding the set of n RGB tuples on a linear scale given all of the n treatments that were collected\n",
    "    initial_colors = linear_generator_starting_colors(len(all_treatments_all_patients))\n",
    "\n",
    "\n",
    "\n",
    "    for treatment_type, each_unlightened_color in zip(all_treatments_all_patients, initial_colors):\n",
    "        starting_colors_dict[treatment_type] = each_unlightened_color\n",
    "    \n",
    "    for index, separated_dataframe in enumerate(separate_dfs):\n",
    "        treatment_series_list = separated_dataframe[\"treatment\"].tolist()\n",
    "        current_treatment = treatment_series_list[0]\n",
    "        treatment_streak_length = 1\n",
    "        last_facecolor = starting_colors_dict[treatment_series_list[0]]\n",
    "        for day, which_treatment in enumerate(treatment_series_list):\n",
    "            bar_width = 1\n",
    "            lightness_factor = max(0.7, 1 - 0.05 * (treatment_streak_length - 1)) #used to darken base facecolors for a given treatment with repeated treatment\n",
    "            if which_treatment == current_treatment:\n",
    "                treatment_streak_length+=1 \n",
    "            else:\n",
    "                current_treatment = which_treatment\n",
    "                treatment_streak_length = 1\n",
    "            \n",
    "            #unblended rectangle's facecolor and plotting\n",
    "            unblended_facecolor = starting_colors_dict[which_treatment]\n",
    "            rect_to_add = rect((day, index), width=1, height=1, facecolor=unblended_facecolor, edgecolor='black', linewidth=0.25)\n",
    "            firstMainPanel.add_patch(rect_to_add)\n",
    "\n",
    "            #blended rectangle's facecolor and plotting\n",
    "            r, g, b = unblended_facecolor[0], unblended_facecolor[1], unblended_facecolor[2]\n",
    "            facecolor_unblended_hls  = rgb_to_hls(r, g, b)\n",
    "            streak_aware_facecolor_unblended = hls_to_rgb(facecolor_unblended_hls[0], facecolor_unblended_hls[1]*lightness_factor, facecolor_unblended_hls[2])\n",
    "            blended_streak_aware_facecolor = blend_old_and_new_color(last_facecolor, streak_aware_facecolor_unblended, alpha)\n",
    "            \n",
    "\n",
    "            blended_rect_to_add = rect((day, index), width=1, height=1, facecolor=blended_streak_aware_facecolor, edgecolor='black', linewidth=0.25)\n",
    "            otherMainPanel.add_patch(blended_rect_to_add)\n",
    "            last_facecolor = blended_streak_aware_facecolor\n",
    "\n",
    "    \n",
    "    # setting limits of first panel and second panel\n",
    "    max_days = max(len(df[\"treatment\"]) for df in separate_dfs)\n",
    "    firstMainPanel.set_xlim(0, max_days)\n",
    "    firstMainPanel.set_ylim(0, len(separate_dfs) - 0.5)\n",
    "    firstMainPanel.set_yticks([index for index in range(len(separate_dfs))])\n",
    "    firstMainPanel.set_yticklabels(all_patient_IDS)\n",
    "    firstMainPanel.invert_yaxis()  \n",
    "\n",
    "    otherMainPanel.set_xlim(0, max_days)\n",
    "    otherMainPanel.set_ylim(0, len(separate_dfs) - 0.5)\n",
    "    otherMainPanel.set_yticks([index for index in range(len(separate_dfs))])\n",
    "    otherMainPanel.set_yticklabels(all_patient_IDS)\n",
    "    otherMainPanel.invert_yaxis()\n",
    "\n",
    "    #adding base colors to the legend panel\n",
    "    legendRight.set_xlim(0,.1)\n",
    "    legendRight.set_ylim(0,len(all_treatments_all_patients))\n",
    "    \n",
    "    which_y = 0\n",
    "    for treatment, starting_color in starting_colors_dict.items():\n",
    "        given_rectangle = rect((0, which_y), width = 1, height = 1, facecolor = starting_color, edgecolor='black', linewidth=0.25)\n",
    "        legendRight.add_patch(given_rectangle)\n",
    "        which_y+=1\n",
    "    legendRight.set_yticks([index+.5 for index in range(len(starting_colors_dict.keys()))])\n",
    "    legendRight.tick_params(axis='y', labelsize=10)\n",
    "    legendRight.set_yticklabels(starting_colors_dict.keys())\n",
    "    \n",
    "    \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "02eddf59-990d-4fde-a9d7-b9b05272e594",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_history_metadata(seperatePatientsDFs, allPatientsIntroDays):\n",
    "\n",
    "    \"\"\"Function that adds a treatment history metadata column to each patient's dataframe by...\n",
    "    1) Loading each seperate patient's dataframe and compute the average baseline severity, normalizing acne severity scores. Then modifies the dataframe, called a severities dataframe.\n",
    "    2) For each, mapping each value for treatment to the a treatment history tuple. It is a tuple of the form ((days of treatment, ai), (days of treatment, ai+1),....(days of treatment, an))\n",
    "    where n is the row number of the current day of the particular treatment.\"\"\"\n",
    "    \n",
    "    \n",
    "    #initializing dict containing severties by day for a given treatment \n",
    "    severitiesDayTreatmentDict = {treatment: None for treatment in seperatePatientsDFs[0][\"treatment\"]}\n",
    "    \n",
    "    modifiedDFs = []\n",
    "    counter = 0 \n",
    "\n",
    "    for patient_DF, days_of_intro in zip(seperatePatientsDFs, allPatientsIntroDays):\n",
    "        #computing average baseline severity for each DF\n",
    "        average_bl = patient_DF[\"AcneSeverity\"].head(days_of_intro[0][2]).mean()\n",
    "        \n",
    "        #forming new dataframe from old one containing percent severity over baseline \n",
    "        modified_DF = patient_DF.copy()\n",
    "        \n",
    "        \n",
    "        modified_DF[\"AcneSeverity\"] = modified_DF[\"AcneSeverity\"].apply(lambda x: (x - average_bl)/average_bl)*100\n",
    "        modifiedDFs.append(modified_DF)\n",
    "        counter += 1\n",
    "    \n",
    "    metadata_DFs = []\n",
    "    all_patients_treatment_histories = []\n",
    "    #iterating over all dataframes and their respective treatment days of introduction\n",
    "    for severities_df, days_of_intro in zip (modifiedDFs, allPatientsIntroDays): \n",
    "        #for each dataframe and the corresponding set of days where a given treatment ends\n",
    "        \n",
    "        #adding an explicit day column to each severities dataframe to enable indexing with df.loc  \n",
    "        severities_df[\"day\"] = range(len(severities_df))\n",
    "    \n",
    "        #initializing the treatment history metadata column\n",
    "        severities_df[\"treatment_history\"] = None\n",
    "    \n",
    "        last_treat_index = 0 #keeps track of which number of the ordered list of treatments is currently being processed\n",
    "        treatment_days = {} #keeping track of how many days each particular treatment goes on for\n",
    "        treatment_history = [] #keeping track of the full history of which treatment occurs before the others and how long they last for\n",
    "        all_treatment_histories = []\n",
    "        #also keeping track of the last treatment\n",
    "        last_treatment_itself = None\n",
    "        \n",
    "        #iterating through rows of the dataframe\n",
    "        \n",
    "        for row_index, row in severities_df.iterrows():\n",
    "            current_day = row[\"day\"]\n",
    "            current_treatment = row[\"treatment\"]\n",
    "                   \n",
    "            #also checking to see if the current treatment is entirely new or falls inside a different treatment block\n",
    "            if current_treatment != last_treatment_itself:\n",
    "                treatment_days[current_treatment] = 1  # First day of new treatment\n",
    "                treatment_history.append((current_treatment, 1))\n",
    "            else:\n",
    "                treatment_days[current_treatment] += 1\n",
    "                treatment_history[-1] = (current_treatment, treatment_days[current_treatment])\n",
    "            \n",
    "            \n",
    "            #once history is built, the history is stored in the original dataframe as an entry in own column\n",
    "            severities_df.at[row_index, \"treatment_history\"] = list(treatment_history)\n",
    "        \n",
    "            transient_history = treatment_history.copy()\n",
    "            all_treatment_histories.append(transient_history)\n",
    "    \n",
    "            last_treatment_itself = current_treatment  \n",
    "            \n",
    "        #also modifying dataframes to remove baseline acne severities, as they've already been used\n",
    "        metadata_DFs.append(severities_df[days_of_intro[0][2]:])\n",
    "        \n",
    "        all_patients_treatment_histories.append(all_treatment_histories)\n",
    "    return metadata_DFs, all_patients_treatment_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "f7b90fe3-6485-49aa-aa16-85435ae09c65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_confidence_intervals(metadata_DFs, quantiles):\n",
    "    \"\"\"Checking the confidence intervals of the quantiles to ensure they don't overlap.\"\"\"\n",
    "    #collecting all severities, converting all to positive values, flattening as we go\n",
    "    all_severities = []\n",
    "    for df in metadata_DFs:\n",
    "        all_severities.extend(df[\"AcneSeverity\"] * -1)\n",
    "    boostrapped_CIs = bootstrap_CI_margin_of_error(all_severities, quantiles)\n",
    "    return boostrapped_CIs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "f9b12034",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_and_plot_severity_states(metadata_DFs):\n",
    "    \"\"\"Determines the quantile cutoff determining the quantiles corresponding to categorical acne severity states (low, medium, and high), by\n",
    "    1) Computing the KDE of the distribution of all normalized acne severity scores over all patients and treatment histories.\n",
    "    2) Using optimization to find the saddle point of the distribution and consolidating with the modes.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, figsize = (5, 5))\n",
    "    \n",
    "    \n",
    "    #collecting all severities, converting all to positive values, flattening as we go\n",
    "    all_severities = []\n",
    "    for df in metadata_DFs:\n",
    "        all_severities.extend(df[\"AcneSeverity\"] * -1)\n",
    "    \n",
    "    #check for normal character by plotting histogram\n",
    "    severities_histo = np.histogram(all_severities, density = True)\n",
    "    #axes.hist(all_severities, bins  = 30, density = True)\n",
    "    axes.set_title(\"Acne Severities Distribution (relative to baseline)\") \n",
    "    axes.set_xlabel(\"Normalized Severity Score\")\n",
    "    axes.set_ylabel(\"Density\")\n",
    "    \n",
    "    \n",
    "    #fitting a kernel density estimate to the data\n",
    "    \n",
    "    sns.kdeplot(all_severities, fill=True)\n",
    "    plt.title(\"KDE of Acne Severity Distribution\")\n",
    "    \n",
    "    #extracting the equation of the pdf and finding the local minimum in between the two modes\n",
    "    kde_pdf = sp.stats.gaussian_kde(all_severities)\n",
    "    \n",
    "    #using max and min of pdf to find saddle point in between 2 modes, sampling 1000 points\n",
    "    severity_grid = np.linspace(np.min(all_severities), np.max(all_severities), 1000)\n",
    "    \n",
    "    neg_kde = lambda x: -kde_pdf(x.reshape(1, -1))\n",
    "    \n",
    "    #finding the two main modes using optimization, with first 2 mode guesses at the .2 and .8 quantiles\n",
    "    guesses = np.percentile(all_severities, [20, 80]) \n",
    "    \n",
    "    modes = []\n",
    "    for guess in guesses:\n",
    "        better = optimize.minimize(neg_kde, np.array([guess])) \n",
    "        modes.append(better.x[0]) \n",
    "    \n",
    "    modes = np.array(modes)\n",
    "    \n",
    "    #finding the saddle point in between the two modes, using that as cutoff for the two patient states\n",
    "    \n",
    "    initial_guess = np.mean(modes) #average of the modes\n",
    "    bds = [(min(modes)+1, max(modes)-1)]  # This is a list of two tuples for each mode\n",
    "    \n",
    "    saddle_pt = optimize.minimize(kde_pdf, [initial_guess], bounds = bds) \n",
    "    state_ranges = [modes[0], saddle_pt.x[0], modes[1]]\n",
    "    state_names = [\"High Severity\", \"Medium Severity\", \"Low Severity\"]\n",
    "    \n",
    "    \n",
    "    #plotting modes and saddle point over the distribution\n",
    "    plt.scatter(modes[0], kde_pdf(modes[0]), color = \"red\", label = \"Lower Mode\")\n",
    "    plt.scatter(modes[1], kde_pdf(modes[1]), color = \"red\", label = \"Upper Mode\")\n",
    "    plt.scatter([saddle_pt.x[0]], kde_pdf([saddle_pt.x[0]]), color='green', label=\"Saddle Point\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return state_names, state_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "af4b582f-752c-46f0-8ce4-8c4192da188b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def assign_states_to_mdfs(metadata_DFs, state_names, orig_state_ranges):\n",
    "    \"\"\"Function to construct and attach a second metadata column onto each patient's dataframe. The column contains\n",
    "    the acne severity categorical state corresponding to a given treatment history.\"\"\"\n",
    "    \n",
    "    new_ranges = [float(orig_state_ranges[0]), float(orig_state_ranges[1]), float(orig_state_ranges[2])]\n",
    "    points_in_each_range = defaultdict(list)\n",
    "    state_and_averages = defaultdict(float)\n",
    "    \n",
    "    for patient_df in metadata_DFs:\n",
    "        severity_states = np.digitize(-1 * patient_df[\"AcneSeverity\"], new_ranges, right = False)\n",
    "        severity_states = np.minimum(severity_states, 2)\n",
    "        patient_df[\"State\"] = [state_names[severity_state] for severity_state in severity_states]\n",
    "\n",
    "    for patient_df in metadata_DFs:\n",
    "        states = patient_df[\"State\"].tolist()\n",
    "        severities = patient_df[\"AcneSeverity\"].tolist()\n",
    "\n",
    "        for index in range(len(patient_df) - 1):\n",
    "            state = states[index]\n",
    "            severity = severities[index]\n",
    "            points_in_each_range[state].append(-1*severity)\n",
    "\n",
    "    for state, points_in_each_range in points_in_each_range.items():\n",
    "        state_and_averages[state] = np.average(points_in_each_range)\n",
    "\n",
    "    \n",
    "    return metadata_DFs, state_and_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "aaa4b1c5-9097-4e44-b2da-67cbdb2827b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def validate_state_assignments_with_bootstrapping_result(metadata_DFs, state_names, orig_state_ranges, boot_middle_qs_dict):\n",
    "    \"\"\"Function that validates if state assignments are valid in the context of hard binning (in particular around\n",
    "    the saddle point).\"\"\"\n",
    "    # Helper: extract the original labels once\n",
    "    # Original state assignments\n",
    "    orig_mdfs = copy.deepcopy(metadata_DFs)\n",
    "    orig_mdfs = assign_states_to_mdfs(orig_mdfs, state_names, orig_state_ranges)\n",
    "    orig_labels = np.concatenate([df[\"State\"].values for df in orig_mdfs])\n",
    "\n",
    "    fracs_changed = {}\n",
    "\n",
    "    # Iterate over numeric middle cutpoints (q̂) in the bootstrap dict\n",
    "    for qhat, boot_info in boot_middle_qs_dict.items():\n",
    "        frac_changed = []\n",
    "\n",
    "        # Iterate over bootstrap samples of the middle cutpoint\n",
    "        for q in boot_info['bootstrap_samples']:\n",
    "            # Ensure numeric\n",
    "            q = float(q)\n",
    "\n",
    "            # Update the middle cutpoint only\n",
    "            low, high = float(orig_state_ranges[0]), float(orig_state_ranges[2])\n",
    "            middle = float(q)\n",
    "            epsilon = 1e-6\n",
    "            middle = max(low + epsilon, min(middle, high - epsilon))\n",
    "            new_ranges = [low, middle, high]\n",
    "\n",
    "            # Reassign states with the adjusted middle cut\n",
    "            mdfs_try = copy.deepcopy(metadata_DFs)\n",
    "            mdfs_try = assign_states_to_mdfs(mdfs_try, state_names, new_ranges)\n",
    "            new_labels = np.concatenate([df[\"State\"].values for df in mdfs_try])\n",
    "\n",
    "            # Fraction of labels that changed\n",
    "            diff_frac = np.mean(new_labels != orig_labels)\n",
    "            frac_changed.append(diff_frac)\n",
    "\n",
    "        frac_changed = np.array(frac_changed)\n",
    "\n",
    "        print(f\"For middle cutpoint q̂={qhat:.4f}\")\n",
    "        print(f\"Median fraction changed: {np.median(frac_changed):.3f}\")\n",
    "        print(f\"5th–95th percentile range: ({np.percentile(frac_changed,5):.3f}, {np.percentile(frac_changed,95):.3f})\\n\")\n",
    "\n",
    "        fracs_changed[qhat] = frac_changed\n",
    "\n",
    "    return fracs_changed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "a9f70894-26f3-4bbc-9f7c-8965c525ac9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_histograms(metadata_DFs):\n",
    "    \"\"\"This algorithm iterates over all patients' severity dataframes and, for each... \n",
    "\n",
    "    1) Counts all of the occcurences of each acne severity state throughout all patients, and assigns those counts \n",
    "    as a value to the treatment history key in the state counts dictionary.\n",
    "    2) Normalizes the counts into distributions before returning both. \n",
    "    \"\"\"\n",
    "    \n",
    "    all_state_counts = defaultdict(Counter)\n",
    "  \n",
    "    for i, patient_df in enumerate(metadata_DFs):\n",
    "        histories = patient_df[\"treatment_history\"].values\n",
    "        states = patient_df[\"State\"].values\n",
    " \n",
    "        for state_index in range(1, len(states)):\n",
    "            current_state = states[state_index] #state at position state_index in the patient's dataframe\n",
    "            current_history = histories[state_index] #metadata (treatment history) at position state_index  in the patient's dataframe\n",
    "            current_history_key = tuple((str(treatment), int(days)) for treatment, days in current_history)\n",
    "           \n",
    "            #recording the actual counts of severities, with the context of the prior treatment as the key\n",
    "            all_state_counts[current_history_key][current_state] += 1\n",
    "\n",
    "    #normalizing counts dictionaries into distributions\n",
    "    first_order_probabilities = {}\n",
    "    \n",
    "    for previous_treatment, state_counts in all_state_counts.items():\n",
    "        total_counts  = sum(state_counts.values())\n",
    "        probabilities_given_previous_state = {state: count/total_counts for state, count in state_counts.items()}\n",
    "        first_order_probabilities[previous_treatment] = probabilities_given_previous_state\n",
    "\n",
    "    return (all_state_counts, first_order_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "f891c9d8-479f-443f-a07d-880fd55b54a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_histograms(first_order_probabilities):\n",
    "    \"\"\" This function plots a striped heatmap to inspect the distributions of normalized acne severity states for each treatment history.\n",
    "    It uses a viridis heatmap implementation from my other repository figuresAndViewers.\n",
    "    In lieu of using the actual treatment histories themselves as x labels, the x label is the index of the history in the sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_dim  = 100\n",
    "    y_dim = 200\n",
    "    \n",
    "    fig = plt.figure(figsize=(x_dim, y_dim))\n",
    "    matplotlib.rc('xtick', labelsize=14)\n",
    "    matplotlib.rc('ytick', labelsize=14)\n",
    "\n",
    "      \n",
    "    #plotting histograms of each context dependent model of acne treatment severity\n",
    "    bar_width = 0.3\n",
    "    spacing = 0.05 \n",
    "    \n",
    "    #sorting the distributions by the state name in reverse \n",
    "    \n",
    "    real_first_order_probabilities = dict(sorted(first_order_probabilities.items(), reverse = True))\n",
    "    \n",
    "    mainPanelHeight = 15\n",
    "    mainPanelWidth = 20\n",
    "\n",
    "    legendPanelHeight = .25\n",
    "    legendPanelWidth = .5\n",
    "    \n",
    "    sidePanelHeight = 3\n",
    "    sidePanelWidth = .25\n",
    "    \n",
    "    \n",
    "    #setting up the panels and placing the proper positions\n",
    "    firstMainPanel = plt.axes([.05/x_dim,.375/y_dim, mainPanelWidth/x_dim, mainPanelHeight/\n",
    "    y_dim])\n",
    "    firstMainPanel.set_xlabel(\"Treatment History Index\")\n",
    "    firstMainPanel.set_title(\"Raw Distributions of Normalized Acne Severity States\")\n",
    "\n",
    "    #setting up the legend panel\n",
    "    legendRight = plt.axes([(1+mainPanelWidth)/x_dim, legendPanelHeight/y_dim, legendPanelWidth/x_dim, mainPanelHeight/y_dim])\n",
    "    #seting ticks of legend\n",
    "    legendRight.tick_params(bottom=False, labelbottom=False, left=True, labelleft=True, right=False, labelright=False, top=False, labeltop=False)\n",
    "    \n",
    "    legendRight.set_xlim(0,.1)\n",
    "    legendRight.set_ylim(0,20)\n",
    "    legendRight.set_yticks([0,20],['0','1'])\n",
    "    \n",
    " \n",
    "    #looping through to construct a heatmap for all distributions\n",
    "    entries = len(real_first_order_probabilities)\n",
    "    bar_width = 1 / entries\n",
    "    firstMainPanel.set_xlim(0, 1)\n",
    "    firstMainPanel.set_ylim(0, 1)\n",
    "\n",
    "    x_pos = 0\n",
    "    for history, raw_distribution in real_first_order_probabilities.items():\n",
    "        distribution = defaultdict(float, raw_distribution)\n",
    "        scaled_x_pos = x_pos * bar_width\n",
    "\n",
    "        high_fc = viridis(distribution[\"High Severity\"])[:3]\n",
    "        med_fc  = viridis(distribution[\"Medium Severity\"])[:3]\n",
    "        low_fc  = viridis(distribution[\"Low Severity\"])[:3]\n",
    "    \n",
    "\n",
    "        firstMainPanel.add_patch(rect([scaled_x_pos, 2/3], width=bar_width, height=1/3, facecolor=high_fc, edgecolor='black', linewidth=0.25))\n",
    "        firstMainPanel.add_patch(rect([scaled_x_pos, 1/3], width=bar_width, height=1/3, facecolor=med_fc, edgecolor='black', linewidth=0.25))\n",
    "        firstMainPanel.add_patch(rect([scaled_x_pos, 0],   width=bar_width, height=1/3, facecolor=low_fc, edgecolor='black', linewidth=0.25))\n",
    "\n",
    "        x_actual_spot = scaled_x_pos + bar_width / 2\n",
    "\n",
    "        x_pos += 1\n",
    "    num_rects = len(real_first_order_probabilities)\n",
    "    tick_positions = [i * bar_width + bar_width / 2 for i in range(num_rects)]\n",
    "\n",
    "    firstMainPanel.set_xticks(tick_positions)\n",
    "    firstMainPanel.set_xticklabels(['' for _ in tick_positions])\n",
    "    firstMainPanel.set_yticks([1/6, 0.5, 5/6])\n",
    "    firstMainPanel.set_yticklabels([\"Low Severity\", \"Medium Severity\", \"High Severity\"], fontsize=15)\n",
    "        \n",
    "    #plotting viridis heatmap in the sidebar\n",
    "    #color map tuple pair linspaces, viridis values\n",
    "    vvLin1Red = np.linspace(68/255, 59/255, 5)\n",
    "    vvLin2Red = np.linspace(59/255, 33/255, 6)\n",
    "    vvLin3Red = np.linspace(33/255, 94/255, 6)\n",
    "    vvLin4Red = np.linspace(94/255, 253/255, 6)\n",
    "    \n",
    "    \n",
    "    vvLin1Green = np.linspace(1/255, 82/255, 5)\n",
    "    vvLin2Green = np.linspace(82/255, 145/255, 6)\n",
    "    vvLin3Green = np.linspace(145/255, 201/255, 6)\n",
    "    vvLin4Green = np.linspace(201/255, 231/255, 6)\n",
    "    \n",
    "    vvLin1Blue = np.linspace(84/255, 139/255, 5)\n",
    "    vvLin2Blue = np.linspace(139/255, 140/255, 6)\n",
    "    vvLin3Blue = np.linspace(140/255, 98/255, 6)\n",
    "    vvLin4Blue = np.linspace(98/255, 37/255, 6)\n",
    "    \n",
    "    \n",
    "    plLin4Red = np.linspace(245/255, 237/255, 5)\n",
    "    plLin3Red = np.linspace(190/255, 245/255, 6)\n",
    "    plLin2Red = np.linspace(87/255, 190/255, 6)\n",
    "    plLin1Red = np.linspace(15/255, 87/255, 6)\n",
    "    \n",
    "    plLin4Green = np.linspace(135/255,252/255, 5)\n",
    "    plLin3Green = np.linspace(48/255, 135/255, 6)\n",
    "    plLin2Green = np.linspace(0/255, 48/255, 6) \n",
    "    plLin1Green = np.linspace(0/255, 0/255, 6)\n",
    "    \n",
    "    plLin4Blue = np.linspace(48/255, 27/255, 5)\n",
    "    plLin3Blue = np.linspace(101/255, 48/255,  6)\n",
    "    plLin2Blue = np.linspace(151/255, 101/255, 6)\n",
    "    plLin1Blue = np.linspace(118/255, 151/255, 6)\n",
    "    \n",
    "    \n",
    "    #total linspaces for all tuple pairs, viridis values\n",
    "    vvListOfRedLins = list(vvLin1Red)+list(vvLin2Red)+list(vvLin3Red)+list(vvLin4Red)\n",
    "    vvListOfGreenLins = list(vvLin1Green)+list(vvLin2Green)+list(vvLin3Green)+list(vvLin4Green)\n",
    "    vvListOfBlueLins = list(vvLin1Blue)+list(vvLin2Blue)+list(vvLin3Blue)+list(vvLin4Blue)\n",
    "    \n",
    "    orderedVVRed = list(dict.fromkeys(vvListOfRedLins))\n",
    "    orderedVVGreen = list(dict.fromkeys(vvListOfGreenLins))\n",
    "    orderedVVBlue = list(dict.fromkeys(vvListOfBlueLins))\n",
    "    \n",
    "    \n",
    "    #viridis heatmaps into the legend panel\n",
    "    for index in range(0,20,1):\n",
    "    \tcolorPaletteVV = (orderedVVRed[index], orderedVVGreen[index], orderedVVBlue[index])\n",
    "    \tvvGradeRect = rect([0,index], .1, 8, facecolor=colorPaletteVV,edgecolor = 'black', linewidth = 0)\t\t\n",
    "    \tlegendRight.add_patch(vvGradeRect)\n",
    "\n",
    "    plt.savefig(\"Raw_Striped_Heatmap\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "3b593e9c-8970-4d5b-8e59-e08154a0d7fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bootstrap_CI_margin_of_error(actual_values, observed_quantile_values, B=5000, bandwidth=0.05):\n",
    "    \"\"\"Short function to approximate the margin of error (MOE) for the bimodal KDE presented here. Uses Bootstrapping to approximate \n",
    "    the 95% confidence interval for each quantile, and halves each of them to give MOE.\"\"\"\n",
    "\n",
    "    data = np.asarray(actual_values)\n",
    "    n = len(data)\n",
    "    kde = KernelDensity(bandwidth=bandwidth).fit(data[:, None])\n",
    "    rng = np.random.default_rng(123)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for qhat in observed_quantile_values:\n",
    "        boot_q = np.empty(B)\n",
    "        # compute percentile of qhat in original data\n",
    "        p = (data < qhat).mean()\n",
    "        for b in range(B):\n",
    "            smpl = kde.sample(n_samples=n, random_state=123 + b).ravel()\n",
    "            boot_q[b] = np.quantile(smpl, p)  # same percentile in resampled data\n",
    "\n",
    "        ci_lower, ci_upper = np.percentile(boot_q, [2.5, 97.5])\n",
    "        moe = 0.5 * (ci_upper - ci_lower)\n",
    "\n",
    "        results[qhat] = {\n",
    "            'CI': (ci_lower, ci_upper),\n",
    "            'MOE': moe,\n",
    "            'bootstrap_samples': boot_q\n",
    "        }\n",
    "\n",
    "        # plot histogram\n",
    "        plt.figure()\n",
    "        plt.hist(boot_q, bins=80)\n",
    "        plt.axvline(qhat, color='k', linestyle='--', label='observed cutpoint')\n",
    "        plt.title(f'Smoothed bootstrap distribution for cutpoint {qhat:.2f}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Cutpoint={qhat:.4f}, 95% CI=({ci_lower:.4f},{ci_upper:.4f}), MOE={moe:.4f}\")\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "16b0c4a7-7d72-4a01-9289-bec85b3aee6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_Dirichlet(prior, all_transition_counts):\n",
    "    \"\"\"This function does the following: \n",
    "    Accepts a prior in order to construct a Bayesian model of each history's distribution as a Dirichlet distribution with a multinomial \n",
    "    likelihood. It does so by the following methods. \n",
    "    1)  Iterates through each set of counts for each history and adds them to each parameter in order in the prior, then uses these to\n",
    "    build the posterior Dirichlet distribution.\"\"\"\n",
    "\n",
    "    categories = ['Low Severity', 'Medium Severity', 'High Severity']\n",
    "    prior_dict = {'Low Severity': prior[0], \"Medium Severity\": prior[1], \"High Severity\": prior[2]}\n",
    "    \n",
    "    history_and_posteriors = {}\n",
    "\n",
    "    \n",
    "    for history, count_dict in all_transition_counts.items():\n",
    "        \n",
    "        counts = [count_dict.get(cat, 0) for cat in categories] #pulling counts from prior dict and counts dict\n",
    "        prior = [prior_dict[cat] for cat in categories] \n",
    "\n",
    "        #updating parameters for posterior distribution\n",
    "        posterior_params = np.array(counts) + np.array(prior)\n",
    "        #saving posterior params to the dictionary\n",
    "        history_and_posteriors[history] = posterior_params\n",
    "     \n",
    "    return history_and_posteriors, categories   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "46c178be-9f94-45ab-b455-6011f2002cff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_dirichlet_marginal_cis(alphas, confidence_level = .95):\n",
    "    \"\"\"Function that is wrapped by the below function. Calculates the upper and lower quantiles supplied by confidence interval\n",
    "    for each marginal beta distribution of a given dirichlet. Returns each confidence interval indexed by the respective alpha.\"\"\"\n",
    "    confidence_intervals = {}\n",
    "    top_density = (1 - confidence_level)/2\n",
    "    bottom_density = 1 - top_density\n",
    "    total_alpha = np.sum(alphas)\n",
    "    \n",
    "\n",
    "    for index, alpha in enumerate(alphas):\n",
    "        other_alpha_sum = total_alpha - alpha\n",
    "        lower_bound = beta.ppf(bottom_density, alpha, other_alpha_sum)\n",
    "        upper_bound = beta.ppf(top_density, alpha, other_alpha_sum)\n",
    "        confidence_intervals[index] = (lower_bound, upper_bound)\n",
    "\n",
    "    return confidence_intervals\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "c5d6430c-3d34-4643-8e74-0e45a68924cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_Dirichlets_credible_interals(histories_and_dirichlets, ordered_categories, confidence_level = .95):\n",
    "    \"\"\"This function finds the 95% credible intervals for each component of the Dirichlet distribution for all treatment histories.\n",
    "    It ensures that a stacked plot is made.\"\"\"\n",
    "    \n",
    "    #checking to see if no categories are supplied; just uses indices of each state to name categories in that case\n",
    "    if ordered_categories is None:\n",
    "        ordered_categories = [f\"State {i}\" for i in range(len(dirichlet_posteriors[0]))]\n",
    "    \n",
    "    fig, ax = plt.subplots(len(ordered_categories), 1, figsize=(len(ordered_categories) * 3.5, 7), sharex = True)\n",
    "    ax[-1].set_xlabel(\"History Index\")\n",
    "    ax[len(ax)-1].set_title(\"95% Credible Intervals for marginal acne severity state distributions\")\n",
    "    jitter_spacing = .5\n",
    "    left_end = 0 \n",
    "    for history, alphas in histories_and_dirichlets.items():\n",
    "        \n",
    "        these_confidence_intervals = find_dirichlet_marginal_cis(alphas)\n",
    "        for subplot_index, ordered_confidence_interval in these_confidence_intervals.items():\n",
    "            #ax[subplot_index].set_xticks(np.arange(len(histories_and_dirichlets)))\n",
    "            #ax[subplot_index].set_xticklabels(list(histories_and_dirichlets.keys()))\n",
    "            x = left_end + (subplot_index - len(these_confidence_intervals)/2) * jitter_spacing  # adding some x offset for the error bars\n",
    "            center = (ordered_confidence_interval[1] + ordered_confidence_interval[0])/2\n",
    "            width = ordered_confidence_interval[0] - ordered_confidence_interval[1]\n",
    "            \n",
    "            ax[len(ordered_categories) - subplot_index - 1].errorbar(x, center, yerr = width/2, fmt='o', color='C0', capsize=5)\n",
    "            \n",
    "        left_end += 1\n",
    "            \n",
    "    for i, name in enumerate(reversed(ordered_categories)):\n",
    "        ax[i].set_ylabel(name)\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "f30a3960-860a-4936-9901-51857787cc69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_KL_Divergence_vectorized(histories_and_posteriors):\n",
    "    \"\"\"This vectorized function calculates the Kullback-Leibler divergence between adjacent \n",
    "    3 dimensional Dirichlet distributions, each of which is indexed by its alpha parameter.\n",
    "    It does this for a full array, computing KL Divergence for each term alpha[i] and alpha[i+1]. So, it requires \n",
    "    you to supply two arrays of parameters, but really, just the same one read forwards and backwards.\n",
    "    \"\"\"\n",
    "    history_labels = list(histories_and_posteriors.keys())\n",
    "    x_vals = np.arange(1, len(history_labels))\n",
    "\n",
    "    alphas = np.array(list(histories_and_posteriors.values()))\n",
    "    alphas_backward = alphas[:-1]\n",
    "    alphas_forward = alphas[1:]\n",
    "\n",
    "    #ensuring alphas are non-0 up front\n",
    "    sum_forward = np.sum(alphas_forward, axis=1)\n",
    "    sum_backward = np.sum(alphas_backward, axis=1)\n",
    "\n",
    "    first_term = gammaln(sum_forward) - gammaln(sum_backward)\n",
    "    second_term = np.sum(gammaln(alphas_backward) - gammaln(alphas_forward), axis=1)\n",
    "    third_term = np.sum(\n",
    "        (alphas_forward - alphas_backward) * \n",
    "        (psi(alphas_forward) - psi(sum_forward)[:, None]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    kl_div = first_term + second_term + third_term\n",
    "    cumulative_kl = np.cumsum(kl_div)\n",
    "    cumulative_kl = np.clip(cumulative_kl, 1e-10, None)  # Avoid log(0)\n",
    "\n",
    "    return x_vals, np.log(cumulative_kl), cumulative_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "c898ff44-216c-4099-88c4-f399172077bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def split_for_piecewise_regression(xs, cumulative_kls, percentile_cutoff = 80, split_index = None, gaussian_sigma = 1, slope_threshold = 0.2,\n",
    "                            min_segment_length = 3):\n",
    "    \"\"\"This algorithm uses a brute force method to find the inflection points to fit piecewise regression models to the data.\n",
    "    But first...\n",
    "    It dynamically chooses the right number of inflection points to fit each model to via the following...\n",
    "    1) It smoothes the cumulative KL divergence array with a Gaussian filter (by default is standard normal). \n",
    "    2) It computes pairwise slopes between adjacent cumulative KL values (with each x interval = 1/number of steps in treatment history)\n",
    "    ^check that later\n",
    "    3) Computes differences between adjacent slopes, returning a 2nd derivative approximation for the entire array\n",
    "    4) (Currently commented out) Plots the distribution of the magnitudes for inspection.\n",
    "    5) A cutoff for inflection points is chosen as a percentile of the 2nd derivative magntitdes (default is 90th). \n",
    "\n",
    "    Then, it iterates through the found inflection points, dividing them into subarrays of consecutive points. Once it does this, it does \n",
    "    one final check to see if each subarray should be further divided. It does this by iterating through each array, comparing the difference in \n",
    "    adjacent slopes pairwise, checking their difference against the provided slope threshold parameter. It then makes a list of lists of \n",
    "    inflection points where the regression models should start and end.\n",
    "\n",
    "    It then uses the result of this to further divide the points into subarrays that a regression model can be fit to.\n",
    "    At last, it checks the length of each subarray against the minimum segment length, and discards ones that are too small.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    smoothed_kls = gaussian_filter1d(cumulative_kls, sigma = gaussian_sigma)\n",
    "    slopes = np.diff(smoothed_kls)\n",
    "    second_derivatives_magnitudes = np.abs(np.diff(slopes))\n",
    "    threshold = np.percentile(second_derivatives_magnitudes, percentile_cutoff)\n",
    "    inflection_points = np.where(second_derivatives_magnitudes > threshold)[0] + 1\n",
    "\n",
    "    differences = np.diff(inflection_points)\n",
    "    non_consecutive_indices = np.where(differences != 1)[0] + 1 \n",
    "    separated_inflection_points = np.array_split(inflection_points, non_consecutive_indices)\n",
    "\n",
    "    \n",
    "    #adding the end of the array to the non_consecutive indices for easier splitting later\n",
    "    #separated_inflection_points[-1] = np.append(separated_inflection_points[len(separated_inflection_points)-1], len(smoothed_kls)-1)    \n",
    "    #^above is causing issues\n",
    "    all_breaks = []\n",
    "\n",
    "    for consecutives_array in separated_inflection_points:\n",
    "        index_consecutives_array = 0\n",
    "        last_inflection_pt = 0 \n",
    "        last_slope = 0\n",
    "\n",
    "        consecutives_breaks = []\n",
    "        \n",
    "        for an_inflection_pt in consecutives_array:\n",
    "            if an_inflection_pt >= len(slopes):\n",
    "                continue\n",
    "            slope_current = slopes[an_inflection_pt]\n",
    "\n",
    "            slope_difference = np.abs(slope_current - last_slope)\n",
    "            \n",
    "            \n",
    "            if slope_difference > slope_threshold:\n",
    "                consecutives_breaks.append(an_inflection_pt)\n",
    "            \n",
    "            last_slope = slope_current\n",
    "            last_inflection_pt = an_inflection_pt\n",
    "\n",
    "        all_breaks.append((consecutives_breaks, index_consecutives_array))\n",
    "\n",
    "        index_consecutives_array += 1 \n",
    "   \n",
    "\n",
    "    fixed_consecutives = []\n",
    "    for breaks, index in all_breaks:\n",
    "        \n",
    "        \n",
    "        if len(breaks) != 0:\n",
    "            consecutives_array_to_split = separated_inflection_points[index]\n",
    "            last_breaking_index = breaks[0] - 1\n",
    "            for breaking_index in breaks:\n",
    "                \n",
    "                broken_consecutives_array = consecutives_array_to_split[last_breaking_index: breaking_index+1]\n",
    "                fixed_consecutives.append(broken_consecutives_array)\n",
    "                last_breaking_index = breaking_index\n",
    "    \n",
    "    #clipping the appropriate array in the inflection point array of arrays \n",
    "    clipped_inflection_points = []\n",
    "    last_consecutive_break = None\n",
    "\n",
    "    # Find the last break if it exists\n",
    "    if fixed_consecutives:\n",
    "        last_consecutive_break = fixed_consecutives[-1][-1]\n",
    "\n",
    "    # Search for that break in the separated inflection points\n",
    "    where_clipping = None\n",
    "    for i, again_consecutives_array in enumerate(separated_inflection_points):\n",
    "        if last_consecutive_break in again_consecutives_array:\n",
    "            idx = np.where(again_consecutives_array == last_consecutive_break)[0][0]\n",
    "            clipped_inflection_points.append(again_consecutives_array[idx + 1:])\n",
    "            where_clipping = i\n",
    "            break  # stop after finding the first match\n",
    "\n",
    "    # Combine final splits\n",
    "    if where_clipping is not None:\n",
    "        full_splits = fixed_consecutives + clipped_inflection_points + separated_inflection_points[where_clipping+1:]\n",
    "    else:\n",
    "        full_splits = fixed_consecutives\n",
    "    return full_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "03e8ea10-e0b9-4c01-bb5a-fd31bece6e90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_piecewise_regression_segments(curve, splits, ax, all_treatment_histories, color = \"blue\", linewidth = 2):\n",
    "    \"\"\"This function is wrapped by piecewise_regression_and_plot below. It plots the actual line segments piecewise\n",
    "    over a rendered cumulative KL Divergence plot. \"\"\"\n",
    "\n",
    "    results_pvals_r2_betas = {}\n",
    "    all_results = []\n",
    "    #for now, just taking the longest treatment history and using that for indexing\n",
    "    full_list_of_histories_all_patients = []\n",
    "    for treatment_history in all_treatment_histories:\n",
    "        for history in treatment_history:\n",
    "             \n",
    "            full_list_of_histories_all_patients.append(tuple(history))\n",
    "    \n",
    "    unique_list_of_histories = list(set(full_list_of_histories_all_patients))\n",
    "    unique_list_of_histories.sort()\n",
    "    \n",
    "    if splits is None:\n",
    "        print(\"No splits to plot!\")\n",
    "        return\n",
    "\n",
    "    #removing any splits of size 7 (too small to be fitted)\n",
    "    removed_splits = [split for split in splits if len(split) > 10]\n",
    "\n",
    "    #setting up subplots \n",
    "    top = len(removed_splits) // 2\n",
    "    bottom = top + (len(removed_splits) % 2)\n",
    "    fig, regression_axes = plt.subplots(2, max(top, bottom), figsize=(20, 20))\n",
    "    fig.suptitle(\"Segments of Non-Diminishing KL Divergence\")\n",
    "    \n",
    "    first_plot_index = 0\n",
    "    all_indices_array = np.arange(0, len(removed_splits))\n",
    "    \n",
    "    \n",
    "    for i, other_splits in enumerate(removed_splits):\n",
    "        fixed_x_ticks = []\n",
    "        \n",
    "        other_split_start, other_split_end = other_splits[0], other_splits[len(other_splits)-1]\n",
    "        these_x_ticks_unfixed = unique_list_of_histories[other_split_start: other_split_end+1]\n",
    "\n",
    "        for other_i in range(len(other_splits)):\n",
    "            joined_history_piece = \" \".join(str(item)+\" \" for item in these_x_ticks_unfixed[other_i])\n",
    "            \n",
    "            fixed_x_ticks.append(joined_history_piece)\n",
    "        \n",
    "        y_segment = curve[other_split_start: other_split_end + 1] \n",
    "        x_segment = np.arange(other_split_start, other_split_end+1).reshape(-1, 1)\n",
    "        regression_piece = LinearRegression().fit(x_segment, y_segment.reshape(-1, 1))\n",
    "        \n",
    "        y_pred = regression_piece.predict(x_segment)\n",
    "        axis_to_plot_on = regression_axes.flatten()[i]\n",
    "        axis_to_plot_on.scatter(x_segment, y_segment)\n",
    "        axis_to_plot_on.plot(x_segment.flatten(), y_pred.flatten(), color=color, linewidth=linewidth)\n",
    "        axis_to_plot_on.set_ylabel(\"KL Divergence (log scale)\")\n",
    "        axis_to_plot_on.set_xlabel(\"Treatment History\")\n",
    "\n",
    "       \n",
    "        \n",
    "        axis_to_plot_on.set_xticks(x_segment.flatten())\n",
    "        \n",
    "        axis_to_plot_on.set_xticklabels(fixed_x_ticks, fontsize = 7, rotation = 45)\n",
    "\n",
    "        #also returning the report for each segement (including p values for slope) using statsmodels package\n",
    "        x = sm.add_constant(x_segment)\n",
    "        this_model = sm.OLS(y_segment.reshape(-1, 1), x).fit()\n",
    "        slope = this_model.params[1]\n",
    "        pval = this_model.pvalues[1]\n",
    "        r2 = this_model.rsquared\n",
    "\n",
    "        results_pvals_r2_betas[i] = {\n",
    "        \"Segment\": i,\n",
    "        \"Start\": (other_split_start, unique_list_of_histories[other_split_start]),\n",
    "        \"End\": (other_split_end, unique_list_of_histories[other_split_end]),\n",
    "        \"Num Points\": len(y_segment),\n",
    "        \"Slope\": float(slope),\n",
    "        \"p-value\": float(pval),\n",
    "        \"R²\": r2}\n",
    "\n",
    "        all_results.append({\n",
    "        \"Segment\": i,\n",
    "        \"Start\": other_split_start, \"Starting History\": unique_list_of_histories[other_split_start], \n",
    "        \"End\": other_split_end, \"Ending History\": unique_list_of_histories[other_split_end], \n",
    "        \"Num Points\": len(y_segment),\n",
    "        \"Slope\": slope.round(4),\n",
    "        \"p-value\": pval,\n",
    "        \"R²\": r2.round(4)})\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df[\"p-value\"] = results_df[\"p-value\"].apply(lambda p: f\"{p:.3e}\" if p < 0.001 else f\"{p:.3f}\")\n",
    "    results_df.to_csv(\"segment_regression_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "f3fed872-9761-48ae-b871-e7e6deb639a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bootstrap_KLD_spline_and_dKL(already_fitted_spline, deriv_ax, x, observed_KLDs, dy_smooth, x_smooth, n_boot = 300):\n",
    "    \"\"\"Another bootstrap algorithm that computes the 95% CIs of the cumulative KL fitted spline and the derivative of it,\n",
    "    based on the residuals of each of n_boot (parameter above) points of the already fitted spline.\n",
    "    The input x refers to the x values used to fit the spline, the input observed_KLDs refers to the real ones.\n",
    "    Unused - dy_smooth is the smoothed derivative of the already fitted spline\"\"\"\n",
    "    \n",
    "    n_boot = 300\n",
    "    y_boot_preds = []\n",
    "    dy_boot_preds = []\n",
    "    \n",
    "    for i in range(n_boot):\n",
    "        # resample residuals\n",
    "        y_fit = already_fitted_spline(x)\n",
    "        residuals = observed_KLDs - y_fit\n",
    "        resampled_y = y_fit + np.random.choice(residuals, size=len(observed_KLDs), replace=True)\n",
    "    \n",
    "        boot_spline = UnivariateSpline(x, resampled_y, s=len(x) * np.var(observed_KLDs) * 0.002)\n",
    "        y_boot_preds.append(boot_spline(x))\n",
    "        dy_boot_preds.append(boot_spline.derivative()(x))\n",
    "    \n",
    "    y_boot_preds = np.array(y_boot_preds)\n",
    "    dy_boot_preds = np.array(dy_boot_preds)\n",
    "    \n",
    "    # Step 1: compute mean derivative across bootstraps\n",
    "    dy_mean = np.mean(dy_boot_preds, axis=0)\n",
    "    \n",
    "    # Step 2: compute deviations from the mean\n",
    "    dy_dev = dy_boot_preds - dy_mean\n",
    "    \n",
    "    # Step 3: compute percentile-based CI of deviations, then recenter around dy_mean\n",
    "    dy_lower = dy_mean + np.percentile(dy_dev, 2.5, axis=0)\n",
    "    dy_upper = dy_mean + np.percentile(dy_dev, 97.5, axis=0)\n",
    "    \n",
    "    # Step 4: plot\n",
    "    #fig, ax = plt.subplots(figsize=(8,5))\n",
    "    deriv_ax.plot(x_smooth, dy_smooth, 'r--', label='Derivative (original fit)')\n",
    "    deriv_ax.fill_between(x, dy_lower, dy_upper, color='red', alpha=0.3, label='95% CI')\n",
    "    deriv_ax.set_xlabel('Treatment index')\n",
    "    deriv_ax.set_ylabel('d(KL)/d(treatment)')\n",
    "    deriv_ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return dy_boot_preds, y_boot_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "3ac4e77d-7ddc-48f5-8a0d-709f9ca3eacb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def determine_dy_intervals_subject_to_threshold(deriv_boot_preds, p_val_threshold = .95):\n",
    "    \"\"\"Function to determine if fraction of bootstrap predictions of the derivative of the cumulative KLD curve\n",
    "    exceeds a threshold for a the amount of slopes that should be so.\"\"\"\n",
    "    deriv_boot_preds = np.array(deriv_boot_preds)  # (n_boot, n_points) \n",
    "    \n",
    "    found_indices = []\n",
    "    for i in range(deriv_boot_preds.shape[1]):\n",
    "        fraction_positive = np.mean(deriv_boot_preds[:, i] > 0)\n",
    "        fraction_negative = np.mean(deriv_boot_preds[:, i] < 0)\n",
    "        if fraction_positive >= p_val_threshold or fraction_negative >= p_val_threshold:\n",
    "            found_indices.append(i)\n",
    "\n",
    "    \n",
    "    dy_splits = split_at_non_consecutives(found_indices)\n",
    "    return dy_splits\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "e988b2a3-e0a8-49b6-a1df-501709ad1754",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def split_at_non_consecutives(arr):\n",
    "    \"\"\"Simple function to split an array into separate arrays of nonconsecutive points.\"\"\"\n",
    "    if not arr:\n",
    "        return []\n",
    "\n",
    "    result = []\n",
    "    current_sub_array = [arr[0]]\n",
    "\n",
    "    for i in range(1, len(arr)):\n",
    "        # Define \"consecutive\" - here, arr[i] is one greater than arr[i-1]\n",
    "        if arr[i] == arr[i-1] + 1:\n",
    "            current_sub_array.append(arr[i])\n",
    "        else:\n",
    "            result.append(current_sub_array)\n",
    "            current_sub_array = [arr[i]]\n",
    "\n",
    "    result.append(current_sub_array)  # Add the last sub-array\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "c6ade13b-c258-449b-a2bb-d44ad151d4ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_KL_Divergence_plot(x_vals, kl_divergences, cumulative_kl, splits, all_treatment_histories):\n",
    "    \"\"\"This function plots the Kullback-Leibler Divergence between each consecutive Dirichlet posterior distribution\n",
    "    as a line plot. This is a wrapper of calculate_KL_Divergence_vectorized.\n",
    "    It also plots the cumulative KL Divergence over the KL divergence between individual distributions.\n",
    "    It ends up plotting the linear regression models over the plot as well. \"\"\"\n",
    "   \n",
    "    # Plotting\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax1.plot(x_vals[0:], kl_divergences[0:], marker='o', label='KL Divergence')\n",
    "    \n",
    "    #approximating curve smoothly with spline\n",
    "    #sorting if needed\n",
    "    sorting_indices = np.argsort(x_vals)\n",
    "    x_to_plot = x_vals[sorting_indices]\n",
    "    cumulative_kls_to_plot = cumulative_kl[sorting_indices]\n",
    "\n",
    "    #fitting smoothed spline, smoothing by a factor of the variance and the number of samples\n",
    "    factor_for_smoothing = 0.002\n",
    "    smoothing = len(x_to_plot) * np.var(cumulative_kls_to_plot) * factor_for_smoothing\n",
    "    fitted_spline = UnivariateSpline(x_to_plot, cumulative_kls_to_plot, s = smoothing)\n",
    "\n",
    "    #evaluating the fitted smoothed spline and its derivative\n",
    "    x_smooth = np.linspace(x_to_plot.min(), x_to_plot.max(), 500)\n",
    "    y_smooth = fitted_spline(x_smooth)\n",
    "    dy_smooth = fitted_spline.derivative()(x_smooth)\n",
    "\n",
    "    ax1.plot(x_smooth, y_smooth, 'o', label='Observed cumulative KL', alpha=0.6)\n",
    "    ax1.plot(x_smooth, y_smooth, 'r-', label='Smoothing spline')\n",
    "    ax1.set_xlabel(\"Treatment index\")\n",
    "    ax1.set_ylabel(\"Cumulative KL divergence\")\n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    # Secondary axis for derivative\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x_smooth, dy_smooth, 'r--')\n",
    "    ax2.set_ylabel(\"d(KL)/d(treatment)\")\n",
    "    ax2.legend(loc='upper right')    \n",
    "\n",
    "    ax1.set_title('KL Divergence and Cumulative Information Gain (Log Scale)')\n",
    "    ax1.set_xlabel('Treatment History Index')\n",
    "    ax1.set_ylabel('KL Divergence (log scale)')\n",
    "    ax1.legend()\n",
    "    \n",
    "\n",
    "    #plotting the confidence intervals of the spline and its derivative using the bootstrap function above\n",
    "    these_dy_boot_preds, these_y_boot_preds = bootstrap_KLD_spline_and_dKL(fitted_spline, ax2, x_to_plot, cumulative_kls_to_plot, dy_smooth, x_smooth)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    these_determined_splits = determine_dy_intervals_subject_to_threshold(these_dy_boot_preds)\n",
    "    linear_models = plot_piecewise_regression_segments(cumulative_kls_to_plot, these_determined_splits, ax1, all_treatment_histories, linewidth = 1)\n",
    "\n",
    "    #mental note: just need to collect the p values for the fit segments, Dubin Watson statistic for autocorrelation, maybe a bootstrap CI\n",
    "    #if not just use the confidence interval spit out by the linear model fitting module above (use chatgpt thread)\n",
    "    \n",
    "    return x_vals, np.log(cumulative_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "f89b4226-a9a9-4df1-b778-fee02135b5bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def data_parsing(data_filename):\n",
    "    \"\"\"Function that does the data parsing, given a csv filename in the same directory.\"\"\"\n",
    "    raw_data = pd.read_csv(data_filename)\n",
    "\n",
    "    #actual implementation\n",
    "\n",
    "    this_seperate_DFs, this_intro_days = seperate_patients(raw_data)\n",
    "    #this_inspected_data = display_plots_of_dataset(this_seperate_DFs, this_intro_days, .1) #for inspection\n",
    "    this_md_DFs, this_treatment_history = add_history_metadata(this_seperate_DFs, this_intro_days)\n",
    "    these_states, these_ranges = find_and_plot_severity_states(this_md_DFs)\n",
    "    these_assigned_md_DFs, these_state_averages = assign_states_to_mdfs(this_md_DFs, these_states, these_ranges)\n",
    "\n",
    "    built_histograms, raw_probabilities = build_histograms(these_assigned_md_DFs)\n",
    "    uninformative_prior = [1,1,1] #with a1 corresponding to low severity, a2 corresponding to medium, and a3 corresponding to high\n",
    "    \n",
    "    these_Dirichlets, these_categories = build_Dirichlet(uninformative_prior, built_histograms)\n",
    "\n",
    "    return this_treatment_history, these_assigned_md_DFs, these_states, these_ranges, this_md_DFs, these_state_averages, these_Dirichlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "6597df55-3c8b-4e3b-a938-fb15a8ce4122",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def data_visualization(these_assigned_md_DFs, this_treatment_history, this_md_DFs, these_ranges):\n",
    "    \"\"\"Function that actually plots all relevant plots for the observed data.\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    these_checked_CIs = check_confidence_intervals(this_md_DFs, these_ranges) #returns a dictionary now\n",
    "    \n",
    "    built_histograms, raw_probabilities = build_histograms(these_assigned_md_DFs)\n",
    "\n",
    "    plotted_histogram = plot_histograms(raw_probabilities)\n",
    "    \n",
    "    uninformative_prior = [1,1,1] #with a1 corresponding to low severity, a2 corresponding to medium, and a3 corresponding to high\n",
    "    \n",
    "    these_Dirichlets, these_categories = build_Dirichlet(uninformative_prior, built_histograms)\n",
    "    \n",
    "    dirichlet_credible_intervals = plot_Dirichlets_credible_interals(these_Dirichlets, these_categories)\n",
    "    \n",
    "    these_xs, this_log_cum_kls, this_kls = calculate_KL_Divergence_vectorized(these_Dirichlets)\n",
    "    splits = split_for_piecewise_regression(these_xs, this_log_cum_kls)\n",
    "    final_plot = make_KL_Divergence_plot(these_xs, this_log_cum_kls, this_kls, splits, this_treatment_history)\n",
    "\n",
    "    return these_Dirichlets, these_categories\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "f6d4b5f1-be9b-4103-a94b-e45f3c3e7258",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def model_building(these_assigned_md_DFs, these_initial_params, raw_distributions, severity_deltas, model_config):\n",
    "    \"\"\"Function that contains all of the models fit to the observed data.\"\"\"\n",
    "    this_fit_model = fit_predictive_linear_regression_model_of_severity(these_assigned_md_DFs)\n",
    "    these_initial_guesses = {}\n",
    "    this_fit_SSM = fit_latent_state_space_model(these_assigned_md_DFs, these_initial_params, raw_distributions, severity_deltas, model_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "0a04df51-3bbd-4c16-913c-7ae1312f15f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_predictive_linear_regression_model_of_severity(metadata_dfs):\n",
    "    \"\"\"Function to fit a predictive linear model of acne severity as a function of: \n",
    "    1) Lagged/previous day's severity; 2/3) Cumulative days of the current treatment (in this case, either antibiotics or cream)\n",
    "    4) Synergistic effect of cream being followed by a certain number of days of antibiotics.\n",
    "    5) Saturation function based on the Michaelis-Menten half saturation constant (particuarly as cream eventually causes the molecular system\n",
    "    of skin cells to approach homeostasis, with its effect progressively leveling off.\"\"\"\n",
    "    \n",
    "    day_and_sev = defaultdict(list)\n",
    "    cream_saturation_constant = 12 #this hyperparameter can be changed as the model learns, at this time it's about half of the cream treatment block\n",
    "    \n",
    "    for metadata_df in metadata_dfs:\n",
    "        for i, row in metadata_df.iterrows():\n",
    "            last_severity = 0 #fix this later to be average baseline\n",
    "            current_history = tuple(row[\"treatment_history\"])\n",
    "\n",
    "\n",
    "            #4 variables - days of current antibiotics, days of current cream, cumulative cream/antibiotic effect, saturation function\n",
    "            #unpacking history to get the relevant pieces, saving as keys\n",
    "            current_treatment = current_history[len(current_history)-1][0]\n",
    "            days_current_treatment = current_history[len(current_history)-1][1]\n",
    "            #finding days of antibiotics if following cream\n",
    "            days_of_current_antibiotics = 0\n",
    "            days_of_current_cream = 0 \n",
    "            antibiotics_cream_interaction = 0\n",
    "            saturation_function_output = 0\n",
    "            \n",
    "            if current_treatment == \"Antibiotics\": #at some point this can be changed away from hardcoding\n",
    "                days_of_current_antibiotics = days_current_treatment\n",
    "                if len(current_history) != 1: #ie, not the first treatment in a series\n",
    "                    last_treatment = current_history[len(current_history)-2][0]\n",
    "                    if last_treatment == \"Cream\": #this part gives syngergistic effect of antibiotics used after cream\n",
    "                        previous_cream_days = current_history[len(current_history)-2][1]\n",
    "                        #simple linear function of the days of antibiotics giving the antibiotics_cream_interaction\n",
    "                        antibiotics_cream_interaction = previous_cream_days * days_of_current_antibiotics \n",
    "                       \n",
    "            if current_treatment == \"Cream\":\n",
    "                days_of_current_cream = days_current_treatment\n",
    "                saturation_function_output = days_of_current_cream/(cream_saturation_constant + days_of_current_cream)\n",
    "\n",
    "            current_severity = row[\"AcneSeverity\"]\n",
    "            model_contributions = ({\"days_of_current_antibiotics\": days_of_current_antibiotics, \"days_of_current_cream\": days_of_current_cream,\n",
    "                             \"antibiotics_cream_interaction\": antibiotics_cream_interaction, \"saturation_function_output\": saturation_function_output}, \n",
    "                                  last_severity, current_severity)\n",
    "            \n",
    "            day_and_sev[current_history].append(model_contributions)\n",
    "            last_severity = current_severity\n",
    "\n",
    "    #saving the independent variable values per history to a dataframe for easier fitting to the multivariate regression model\n",
    "    rows = []\n",
    "    for history, entries in day_and_sev.items():\n",
    "        #print(entries)\n",
    "        for features, previous_sev, curr_sev in entries:\n",
    "            delta = curr_sev - previous_sev\n",
    "            row = features.copy()\n",
    "            row['delta_severity'] = delta\n",
    "            row['treatment_history'] = str(history) \n",
    "            rows.append(row)\n",
    "    \n",
    "    dataframe_for_fitting = pd.DataFrame(rows)\n",
    "    history_vectors_of_independent_vars = dataframe_for_fitting[['days_of_current_antibiotics', 'days_of_current_cream',\n",
    "        'antibiotics_cream_interaction', 'saturation_function_output']]\n",
    "    history_vectors_of_independent_vars = sm.add_constant(history_vectors_of_independent_vars) #adding constant term\n",
    "    severity_change = dataframe_for_fitting[\"delta_severity\"]\n",
    "\n",
    "    fitted_model = sm.OLS(severity_change, history_vectors_of_independent_vars).fit()\n",
    "    #print(fitted_model.summary())    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b34bb0-a6b9-4949-8b43-881932592655",
   "metadata": {},
   "source": [
    "\\documentclass{article}\n",
    "\\usepackage{amsmath}\n",
    "\\usepackage{amsfonts}\n",
    "\\usepackage{amssymb}\n",
    "\\usepackage{booktabs}\n",
    "\\begin{document}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee59d00-1407-458b-a723-60a2054a6bb5",
   "metadata": {},
   "source": [
    "# Model Structure and Parameter Estimation\n",
    "\n",
    "\n",
    "### Latent State and Associated Evolution\n",
    "The inferred Latent State Vector is  $v_{t} = \\begin{pmatrix}B_t & I_t & S_t\\end{pmatrix}^{T}$, where $B_t$, $I_t$, and $S_t$ refers to bacterial presence, inflammatory activity, and sebum production, respectively, at time t. The components evolve according to the following vector-valued function, $v_{t} = F_{\\theta}(v_{t-1}, u_{t-1}; \\theta) + w_{t}$: \n",
    "\n",
    "$\n",
    "\\begin{array}{lcc}\n",
    "\\text{Component} & \\text{Equation} \\\\\n",
    "\\hline\n",
    "B_{t} & B_{t-1} + r_{growth} \\cdot B_{t-1}\\frac{1-B_{t-1}}{K_{CC}}-k_{antibiotics} \\cdot days_{antibiotics} \\cdot B_{t-1} + k_{sebum} \\cdot B_{t-1} \\cdot S_{t-1} + noise \\\\\n",
    "I_{t} & I_{t-1} + I_{bacterial \\, induction} \\cdot B_{t-1} - I_{decay}/T(tstd)\\cdot T(tstd) - I_{baseline decay} \\cdot I_{t-1} + noise \\\\\n",
    "S_{t} & S_{t-1} + r_{I production} \\cdot I_{t-1} -r_{cream \\, clean} \\cdot cream \\, used + noise\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "Where $r_{growth}$ refers to the growth constant of acne causing bacteria, $K_{CC}$ refers to their carrying capacity, $k_{antibiotics}$ refers to the antibiotic's action rate constant, $k_{sebum}$ refers to the action constant of sebum increasing bacterial load, $I_{bacterial \\, induction}$ refers to the rate of inflammation induced by bacterial load, $I_{decay}/tstd$ refers to the proportionality constant between inflammation reduction and cumulative treatment effect, $T(tstd)$. $r_{I production}$ refers to the rate of inflammation increase given active inflammation, $r_{cream \\, clean}$ refers to the amount of mechanical sebum removal that the cream causes. \n",
    "\n",
    "$T(tstd)$ refers to the cumulative treatment effect of a given treatment history (referred to here as treatment series to date, tstd). $T(tstd)$ is calculated as the expected acne severity change over the Dirichlet posterior distribution corresponding to the last day in the treatment series to date, that is: $T(tstd) = \\mathbb{E}(\\Delta Severity \\mid tstd)$  \n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07efa9a-c762-4810-b81b-3de688868112",
   "metadata": {},
   "source": [
    "### Mapping from Latent State to Acne Severity Change State Probability\n",
    "\n",
    "\n",
    "Here, the probability distribution of Acne Severity Change State (Low, Medium, and High) is a categorical distribution given by an affine function of $v_t$, that is, $Pr(Acne\\,\\,Severity\\,\\,Change\\,\\,State_{t} \\mid v_{t}) = softmax(Wv_{t} + b)$ = \n",
    "$softmax(\\begin{pmatrix} w_{B,\\,S1} & w_{I,\\,S1} & w_{S,\\,S1} \\\\\\\\ w_{B,\\,S2} & w_{I,\\,S2} & w_{B,\\,S2} \\\\\\\\ w_{B,\\,S3} & w_{I,\\,S3} & w_{B,\\,S3} \\end{pmatrix} \\cdot \\begin{pmatrix} B_{t} \\\\ I_{t} \\\\ S_{t} \\end{pmatrix} + \\begin{pmatrix} b_{S1} \\\\ b_{S2} \\\\ b_{S3} \\end{pmatrix})$\n",
    "$$ $$\n",
    "Here, $W$ refers to a 3x3 matrix, whose ijth entry is the corresponding weight, and $B$ refers to a 1x3, whose jth entry refers to the corresponding biases below.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b4f36-1eab-49ae-828c-5511ffe1512a",
   "metadata": {},
   "source": [
    "### Expectation-Maximization of Model Parameters and Latent State Acne Severity Change State Distribution\n",
    " \n",
    "\n",
    "As with typical EM algorithms, this implementation maximizes a log-likelihood function. We posit that the observed acne severity change change Dirichlet distribution for history t is generated by: \n",
    "1) A latent state trajectory: the set of all latent state vectors over all histories up to t.  \n",
    "2) An an underlying mapping from latent state to probabilities of each acne severity change state being realized (see above).\n",
    "\n",
    "\n",
    "Since each datum is a vector of the 3 alpha parameters for the Dirichlet distribution associated with the given history, its probability of occurence, given the expected probabilities of each severity change state occuring above, is:  \n",
    "$$ L(\\theta \\mid History_{t})= \\Pr(History \\,t \\mid \\theta) = \\prod_{k = 0}^{2} \\mathbf{(Pr(Low, \\, Medium, \\, High) \\mid v_{t}})_{k}\\mathbf(Pr(Acne\\,Severity\\,Change\\,State_{k})$$\n",
    "Which is, in other words: $\\mathop{{}\\mathbb{E}(Pr(Predicted\\,Distribution = Observed)}$, or the negative cross entropy: -H(predicted,observed).\n",
    "\n",
    "Given these probabilities, the expected log-likelihood of the entire data series over all histories is (with IID assumption of histories) is the sum of the logs of each one. This calculation completes the expectation step. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879d7f3-f448-48bc-87b4-06f2574cbc93",
   "metadata": {},
   "source": [
    "$$\\textbf{Maximization of Parameters}$$\n",
    "\n",
    "The maximization step maximizes both the weights/biases and the model parameters (that is, $\\theta$). Since there is no closed form for the likelihood of the data given the existing weights $ L(Acne\\,\\,Severity\\,\\,Change\\,\\,State \\mid W_{i}, B_{i})$, the model maximizes it with gradient ascent, according to the following gradient formulations:\n",
    "$$\\frac{\\partial L}{\\partial W} = \\sum_{i=1}^{number\\,of\\,histories} (Pr(Low, Medium, High \\mid v_{t}) - Pr(Low, Medium, High)_{k})v_{k}$$\n",
    "and $$\\frac{\\partial L}{\\partial B} = \\sum_{i=1}^{number\\,of\\,histories} (Pr(Low, Medium, High \\mid v_{t}) - Pr(Low, Medium, High)_{k})$$\n",
    "\n",
    "In the case of the model parameters $(\\theta)$, each of them are optimized via typical Ordinary Least Squares, that is, \n",
    "$\\hat{\\theta_{k}} = (X^{T}X)^{-1}X^{T}Y$, where $Y$ is the predicted $kth$ components of all predicted latent states, and $X$ is the matrix containing the terms multiplying the kth component. To prevent errors from inversion of singular matrices, the Moore-Penrose Pseudoinverse is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "adf21345-0410-4679-8354-67cea47753b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def state_evolution_vv(v_t_last, params, history, raw_distribution, severity_deltas):\n",
    "    \"\"\"Explicit function for the state evolution function, F_theta.\n",
    "    Computes the next latent state, tstd. \n",
    "    Then, it returns them along with the days of antibiotics used up to treatment day t and either 1 or 0 if cream was used on day t.  \"\"\"\n",
    "    prev_bac, prev_inf, prev_sebum = v_t_last\n",
    "    distribution_alphas = np.array(raw_distribution)\n",
    "    probs = distribution_alphas / np.sum(distribution_alphas)\n",
    "    # ensuring severity deltas is a 1D numeric array\n",
    "    severity_array = np.array(list(severity_deltas.values()) if isinstance(severity_deltas, dict) else severity_deltas, dtype=float)\n",
    "    tstd_term = np.sum(probs * severity_array)\n",
    "\n",
    "\n",
    "    days_antib = history[-1][1] if history[-1][0] == \"Antibiotics\" else 0\n",
    "    was_cream_used = 1 if history[-1][0] == \"Cream\" else 0\n",
    "\n",
    "    cur_bac = prev_bac + params[\"r_growth\"]*prev_bac*((1-prev_bac)/params[\"K_CC\"]) \\\n",
    "              - params[\"k_antibiotics\"]*days_antib*prev_bac \\\n",
    "              + params[\"k_sebum\"] * prev_bac * prev_sebum\n",
    "\n",
    "    cur_inf = prev_inf + params[\"I_bacterial_induction\"]*prev_bac \\\n",
    "              - params[\"I_decay_tstd\"]*tstd_term \\\n",
    "              - params[\"I_baseline_decay\"]*prev_inf\n",
    "\n",
    "    cur_sebum = prev_sebum + params[\"r_I_production\"]*prev_inf \\\n",
    "                - params[\"r_cream_clean\"]*was_cream_used\n",
    "\n",
    "    #clipping each returned value to avoid computational issues\n",
    "    cur_bac = np.clip(cur_bac, 0, 10)\n",
    "    cur_inf = np.clip(cur_inf, 0, 10)\n",
    "    cur_sebum = np.clip(cur_sebum, 0, 10)\n",
    "\n",
    "    return np.array([cur_bac, cur_inf, cur_sebum]), tstd_term, days_antib, was_cream_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "4539424b-16c3-415f-98fe-d632d039a795",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def log_prior(model_params):\n",
    "    \"\"\"Function that leverages MAP estimation to assign log normal priors to growth/decay rate parameters and weak L2 ones to others,\n",
    "    aiming to improve convergence.\"\"\"\n",
    "    lp = 0.0\n",
    "    # log-normal priors\n",
    "    lp -= 0.5 * (np.log(model_params[\"r_growth\"]) / 1.0)**2\n",
    "    lp -= 0.5 * (np.log(model_params[\"K_CC\"]) / 1.0)**2\n",
    "    lp -= 0.5 * (np.log(model_params[\"k_antibiotics\"]) / 1.0)**2\n",
    "    lp -= 0.5 * (np.log(model_params[\"k_sebum\"]) / 1.0)**2\n",
    "    lp -= 0.5 * (np.log(model_params[\"I_bacterial_induction\"]) / 1.0)**2\n",
    "    lp -= 0.5 * (np.log(model_params[\"I_decay_tstd\"]) / 1.0)**2\n",
    "    lp -= 0.5 * (np.log(model_params[\"I_baseline_decay\"]) / 1.0)**2\n",
    "    lp -= 0.5 * (np.log(model_params[\"r_I_production\"]) / 1.0)**2\n",
    "    lp -= 0.5 * (np.log(model_params[\"r_cream_clean\"]) / 1.0)**2    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "847a51f9-b0c2-4b11-a5fd-0636d9e308ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood(raw_distributions, pred_state_vectors, parameters):\n",
    "    \"\"\"This function is wrapped by optimize_parameters. It uses maximum-likelihood estimation\n",
    "    to estimate the variance in the residuals between empirical distributions of acne severity state and distributions of\n",
    "    acne severity state conditioned on a latent state.\"\"\"\n",
    "\n",
    "    total_log_l = 0.0\n",
    "    for history, raw_distribution in raw_distributions.items():\n",
    "        predicted_vector_this_history = pred_state_vectors[history]  \n",
    "        predicted_probs = map_latent_states_to_severity_probs(predicted_vector_this_history, parameters)\n",
    "        \n",
    "        # avoid log(0)\n",
    "        predicted_probs = np.clip(predicted_probs, 1e-10, 1.0)\n",
    "        \n",
    "        distribution_alphas = np.array(raw_distribution)\n",
    "        dirichlet_probs = distribution_alphas / np.sum(distribution_alphas)\n",
    "        \n",
    "        # Expected log-likelihood contribution for this history\n",
    "        logL_history = np.sum(dirichlet_probs * np.log(predicted_probs))\n",
    "        total_log_l += logL_history\n",
    "    \n",
    "    return total_log_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "d5c4f97f-ca7a-4298-bd09-1ec1e30e6d33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def softmax(score):\n",
    "        \"\"\"Standalone softmax function.\"\"\"\n",
    "        exp_score = np.exp(score - np.max(score))  \n",
    "        return exp_score / np.sum(exp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "75c5e7e9-e279-4890-a315-30395a37345b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def map_latent_states_to_severity_probs(latent_state, scoring_hyperparameters):\n",
    "    \"\"\"Function used in the expectation step. It maps a latent state to a distribution over the 3 discerete acne severity change states.\n",
    "    The distribution is produced with a weighted sum (score) of how coupled each severity state is to each of the components of the latent state. \n",
    "    Softmaxing converts scores into probabilities.\"\"\"\n",
    "\n",
    "    def softmax(x):\n",
    "        # subtract max for numerical stability\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "    # Get weights and biases\n",
    "    W = np.array(scoring_hyperparameters[\"scoring\"], dtype=float)  # shape (3,3)\n",
    "    b = np.array(scoring_hyperparameters[\"biases\"], dtype=float)   # shape (3,)\n",
    "\n",
    "    # Normalize latent state to prevent huge values\n",
    "    latent_norm = latent_state / (np.linalg.norm(latent_state) + 1e-6)\n",
    "\n",
    "    # Linear scoring\n",
    "    score = W @ latent_state + b\n",
    "\n",
    "    # Clip scores before softmax\n",
    "    score = np.clip(score, -10, 10)\n",
    "\n",
    "    # Softmax to probabilities\n",
    "    probabilities = softmax(score)\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "4228261a-3cca-411c-92d7-402a007a6456",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def smm_model(prev_state, params, raw_distributions, severity_deltas, index):\n",
    "    \"\"\"Wrapper for state_evolution_vv. Uses state_evolution_vv to evaluate the model's Jacobian at the previous state\n",
    "    for propagation of state uncertainity in Kalman Filtering. \n",
    "    Returns that output along with the outputs of state_evolution_vv at the previous state.\"\"\"\n",
    "    history, raw_distribution = list(raw_distributions.items())[index]\n",
    "\n",
    "    # Compute evolution (next latent state)\n",
    "    evolution_output, tstd_term, days_antib, was_cream_used = state_evolution_vv(prev_state, params, history, raw_distribution, severity_deltas)\n",
    "    #evolution output is the predicted next latent state\n",
    "    return evolution_output, tstd_term, days_antib, was_cream_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "2933c7e2-01be-4972-8545-7c8e57627aef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(raw_distributions, predicted_states, scoring_hyperparams, learning_rate = 1e-4, maximum_steps = 30, clip_value = 5.0):\n",
    "    \"\"\"Function that implements gradient ascent to maximize the log-likelihood of the model's hyperparameters\n",
    "    used in map_latent_states_to_severity_probs.\"\"\"\n",
    "    lls = [] #log likelihoods\n",
    "    # ensure float arrays\n",
    "    scoring_hyperparams[\"scoring\"] = np.array(scoring_hyperparams[\"scoring\"], dtype=float)\n",
    "    scoring_hyperparams[\"biases\"] = np.array(scoring_hyperparams[\"biases\"], dtype=float)\n",
    "    def compute_gradient(raw_distributions, predicted_latent_states, parameters):\n",
    "        \n",
    "        \"\"\"Inner function for computing gradient of log-likelihood function with respect to scoring weights and biases.\"\"\"\n",
    "        num_states = len(list(raw_distributions.values())[0])\n",
    "        grad_W = np.array(scoring_hyperparams[\"scoring\"], dtype = float)\n",
    "        grad_b = np.array(scoring_hyperparams[\"biases\"], dtype = float)\n",
    "\n",
    "        for history, raw_alphas in raw_distributions.items():\n",
    "            v_h = predicted_latent_states[history]\n",
    "            \n",
    "            P = map_latent_states_to_severity_probs(v_h, parameters)  # softmax probs\n",
    "            distribution_alphas = np.array(raw_alphas)\n",
    "            empirical_probs = distribution_alphas/np.sum(distribution_alphas)\n",
    "            grad_W += np.outer(empirical_probs - P, v_h)\n",
    "            grad_b += (empirical_probs - P)\n",
    "\n",
    "        #clipping gradients to prevent issues\n",
    "        grad_W = np.clip(grad_W, -clip_value, clip_value)\n",
    "        grad_b  = np.clip(grad_b, -clip_value, clip_value)\n",
    "\n",
    "        return {\"scoring\": grad_W, \"biases\": grad_b}\n",
    "    \n",
    "    \n",
    "    #actual execution\n",
    "    for iteration in range(maximum_steps):\n",
    "        gradients = compute_gradient(raw_distributions, predicted_states, scoring_hyperparams)\n",
    "        scoring_hyperparams[\"scoring\"] += (float(learning_rate) * gradients[\"scoring\"])\n",
    "        scoring_hyperparams[\"biases\"] += (float(learning_rate) * gradients[\"biases\"])\n",
    "        ll = compute_log_likelihood(raw_distributions, predicted_states, scoring_hyperparams)\n",
    "        lls.append(ll)\n",
    "\n",
    "    return scoring_hyperparams, lls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "b2d7ac40-7cf6-4f92-9b1d-d5ccd0613bf7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_process_and_measurement_covariances(pred_state_vectors, model_params, scoring_hyperparams, raw_distributions, severity_deltas):\n",
    "    \"\"\"\n",
    "    Optimize the process (Q) and measurement (R) noise covariances.\n",
    "    pred_state_vectors: dict of {history_name: latent_state_vector} from the E-step\n",
    "    model_parameters: dict of current model parameters\n",
    "    raw_distributions: dict of observed Dirichlet distributions\n",
    "    severity_deltas: dict of severity changes\n",
    "    Uses partials to make call to state evolution function above more straightforward. \n",
    "    \"\"\"\n",
    "    \n",
    "    #creating a partial of smm_model that fixes params, raw_distributions, severity_deltas\n",
    "    state_evolution_fn = partial(\n",
    "        smm_model,\n",
    "        params=model_params,\n",
    "        raw_distributions=raw_distributions\n",
    "    )\n",
    "    \n",
    "    # ---- Process covariance Q ----\n",
    "    q_residuals = []\n",
    "    for history_index, (history, v_t) in enumerate(pred_state_vectors.items()):\n",
    "        evolution_output, _, _, _, _, _ = state_evolution_fn(prev_state=v_t, severity_deltas=severity_deltas, index=history_index)\n",
    "        q_residuals.append(v_t - evolution_output)\n",
    "    \n",
    "    q_residuals = np.stack(q_residuals)\n",
    "    Q = np.cov(q_residuals, rowvar=False)\n",
    "\n",
    "    # ---- Measurement covariance R ----\n",
    "    r_residuals = []\n",
    "    for history_index, (history, v_t) in enumerate(pred_state_vectors.items()):\n",
    "        y_obs = np.array(raw_distributions[history])\n",
    "        y_obs = y_obs / np.sum(y_obs)  # normalize to probabilities\n",
    "        y_pred = map_latent_states_to_severity_probs(v_t, scoring_hyperparams)\n",
    "        r_residuals.append(y_obs - y_pred)\n",
    "    \n",
    "    r_residuals = np.stack(r_residuals)\n",
    "    R = np.cov(r_residuals, rowvar=False)\n",
    "    \n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "0e2f2dc1-5016-4c6d-b89f-613d1416cfc5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_a_linear_parameter_set(target_values,input_matrix,mu=0.0,sigma=1.0,lognormal=True):\n",
    "    \"\"\"Function that uses ordinary least squares to optimize a set of linear parameter from the model that correspond to one component.\n",
    "    Now enforces positivity by imposing a log normal prior (unless configured otherwise) onto linear parameters.\"\"\"\n",
    "    # OLS estimate\n",
    "    theta = np.linalg.pinv(input_matrix) @ target_values\n",
    "\n",
    "    # enforce positivity\n",
    "    theta = np.maximum(theta, 1e-8)\n",
    "\n",
    "    if lognormal:\n",
    "        log_theta = np.log(theta)\n",
    "        penalty_grad = log_theta / sigma**2\n",
    "        theta -= 0.01 * penalty_grad   # small step toward prior mode\n",
    "\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "a81a8c79-5c06-49de-b799-0a79b587d949",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def full_maximimzation_step(\n",
    "        pred_state_vectors,\n",
    "        raw_distributions,\n",
    "        parameters,\n",
    "        model_params,\n",
    "        state_evolution_function,\n",
    "        prev_state_vectors,\n",
    "        days_antibiotics,\n",
    "        t_tstds,\n",
    "        cream_useds,\n",
    "        severity_deltas, \n",
    "        learning_rate=1e-4,\n",
    "        max_grad_steps=30):\n",
    "\n",
    "    # 1. Optimize nonlinear scoring weight and bias hyperparameters\n",
    "    scoring_params, scoring_ll = optimize_hyperparameters(\n",
    "        raw_distributions=raw_distributions,\n",
    "        predicted_states=pred_state_vectors,\n",
    "        scoring_hyperparams = parameters,\n",
    "        learning_rate=learning_rate,\n",
    "        maximum_steps=max_grad_steps\n",
    "    )\n",
    "    parameters[\"scoring\"] = scoring_params[\"scoring\"]\n",
    "    parameters[\"biases\"]  = scoring_params[\"biases\"]\n",
    "\n",
    "    # Helper function to safely slice lists to match target length\n",
    "    def safe_slice(lst, target_len):\n",
    "        return lst[:target_len] if len(lst) > target_len else lst\n",
    "\n",
    "    # 3a. Bacterial component\n",
    "    bac_targets = np.array([v[0] for v in pred_state_vectors.values()])\n",
    "    bac_inputs = np.column_stack([\n",
    "        [v_prev[0] for v_prev in prev_state_vectors.values()],\n",
    "        safe_slice(days_antibiotics, len(bac_targets)),\n",
    "        [v_prev[2] for v_prev in prev_state_vectors.values()]\n",
    "    ])\n",
    "     \n",
    "    new_r_growth, new_k_antibiotics, new_k_sebum = optimize_a_linear_parameter_set(bac_targets, bac_inputs)\n",
    "    model_params[\"r_growth\"]      = new_r_growth\n",
    "    model_params[\"k_antibiotics\"] = new_k_antibiotics\n",
    "    model_params[\"k_sebum\"]       = new_k_sebum\n",
    "\n",
    "    # 3b. Inflammation component\n",
    "    inf_targets = np.array([v[1] for v in pred_state_vectors.values()])\n",
    "    inf_inputs = np.column_stack([\n",
    "        [v_prev[0] for v_prev in prev_state_vectors.values()],\n",
    "        safe_slice(t_tstds, len(inf_targets)),\n",
    "        [v_prev[1] for v_prev in prev_state_vectors.values()]\n",
    "    ])\n",
    "    new_I_bac_induction, new_I_decay_tstd, new_I_baseline_decay = optimize_a_linear_parameter_set(inf_targets, inf_inputs)\n",
    "    model_params[\"I_bacterial_induction\"] = new_I_bac_induction\n",
    "    model_params[\"I_decay_tstd\"]          = new_I_decay_tstd\n",
    "    model_params[\"I_baseline_decay\"]      = new_I_baseline_decay\n",
    "\n",
    "    # 3c. Sebum component\n",
    "    seb_targets = np.array([v[2] for v in pred_state_vectors.values()])\n",
    "    seb_inputs = np.column_stack([\n",
    "        [v_prev[1] for v_prev in prev_state_vectors.values()],\n",
    "        safe_slice(cream_useds, len(seb_targets))\n",
    "    ])\n",
    "    new_r_I_production, new_r_cream_clean = optimize_a_linear_parameter_set(seb_targets, seb_inputs)\n",
    "    model_params[\"r_I_production\"] = new_r_I_production\n",
    "    model_params[\"r_cream_clean\"]  = new_r_cream_clean\n",
    "\n",
    "    return parameters, model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "1fe9d684-9833-43f7-851c-541fca3c88ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_observation_Jacobian_softmax(weights, biases, v_t):\n",
    "    \"\"\"Function that computes the Jacobian for Kalman gain, given the mapping between acne severity change state probability \n",
    "    and latent state.\"\"\"\n",
    "    predicted_probs = weights @ v_t + biases\n",
    "    predicted_probs_softmaxed =  np.exp(predicted_probs - np.max(predicted_probs))/np.sum(np.exp(predicted_probs - np.max(predicted_probs)))\n",
    "    #Jacobian\n",
    "    J_softmax = np.diag(predicted_probs_softmaxed) - np.outer(predicted_probs_softmaxed, predicted_probs_softmaxed)\n",
    "    J_obs_softmax = J_softmax @ weights.T\n",
    "    return J_obs_softmax\n",
    "    \n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "664896b5-85f5-4422-a1f7-d39ac1d566b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_latent_state_space_model(metadata_dfs, initial_parameters, raw_distributions, severity_deltas, model_config, max_iterations=20):\n",
    "    \"\"\"EM-Extended Kalman Filter fitting for acne latent state space model.\"\"\"\n",
    "\n",
    "    # --- Initial guesses for latent states and covariances---\n",
    "    initial_states_guess = np.array([1.0, 1.0, 1.0])  # bacteria, inflammation, sebum\n",
    "    #initial_covariance = np.diag([0.01, 0.01, 0.01])  # initial uncertainties\n",
    "\n",
    "    # --- Last iteration values ---\n",
    "    #scoring hyperparams for mapping of latent state to probability distribution\n",
    "    #updating to now have 3 weights per acne severity change state instead of just vector of probabilities of each state\n",
    "    last_state_weights = np.array(model_config[\"scoring\"])\n",
    "    last_state_biases = np.array(model_config[\"biases\"])\n",
    "    \n",
    "    #last_Q = model_config[\"Q\"] \n",
    "    #last_R = np.maximum(model_config[\"R\"], 1e-3 * np.eye(3)) #changed to have a floor\n",
    "    \n",
    "    last_params = initial_parameters.copy()  # model parameters\n",
    "\n",
    "    # --- Initialize for EM ---\n",
    "    histories = list(raw_distributions.keys())\n",
    "    first_history = histories[0]\n",
    "\n",
    "    # Run smm_model for the first history\n",
    "    z_first, t_first, days_first, cream_first = smm_model(initial_states_guess, last_params, raw_distributions, severity_deltas, 0)\n",
    "\n",
    "    # Store initial predictions\n",
    "    v_predictions = {first_history: z_first}\n",
    "    #covariance_predictions = {first_history: initial_covariance}\n",
    "    t_tstds = [t_first]\n",
    "    days_antibiotics = [days_first]\n",
    "    cream_useds = [cream_first]\n",
    "\n",
    "    last_state = z_first\n",
    "    #last_uncertainties = initial_covariance\n",
    "    last_predictions = {h: initial_states_guess.copy() for h in histories}\n",
    "    print(\"ok\")\n",
    "    # ---- EM ----\n",
    "    for em_iteration in range(max_iterations):\n",
    "        t_tstds = []\n",
    "        days_antibiotics = []\n",
    "        cream_useds = []\n",
    "        v_predictions = {h: last_predictions[h].copy() for h in histories}\n",
    "        for history_index, current_history in enumerate(histories):\n",
    "            #-----  Expectation Step\n",
    "            current_history = histories[history_index]\n",
    "            #current state's values (treatment day t) are found here\n",
    "            #last_predictions[current_history]\n",
    "            z_current_prediction, predicted_tstd_term, here_days_antib, here_was_cream_used = smm_model(last_predictions[current_history], last_params, raw_distributions, severity_deltas, history_index)\n",
    "            \n",
    "            v_predictions[current_history] = z_current_prediction\n",
    "            last_predictions[current_history] = z_current_prediction  #updating prev_state_vectors \n",
    "            \n",
    "            t_tstds.append(predicted_tstd_term)\n",
    "            days_antibiotics.append(here_days_antib)\n",
    "            cream_useds.append(here_was_cream_used)\n",
    "\n",
    "        # ---- Maximization Step ----\n",
    "        # Create a partial of smm_model for Q/R optimization\n",
    "        state_evolution_fn = partial(\n",
    "            smm_model,\n",
    "            params=last_params,\n",
    "            raw_distributions=raw_distributions,\n",
    "            severity_deltas=severity_deltas\n",
    "        )\n",
    "\n",
    "        updated_parameters, last_params = full_maximimzation_step(\n",
    "            pred_state_vectors=v_predictions,\n",
    "            raw_distributions=raw_distributions,\n",
    "            parameters={\"scoring\": last_state_weights, \"biases\": last_state_biases},\n",
    "            model_params=last_params,\n",
    "            state_evolution_function=state_evolution_fn,\n",
    "            prev_state_vectors=last_predictions,\n",
    "            days_antibiotics=days_antibiotics,\n",
    "            t_tstds=t_tstds,\n",
    "            cream_useds=cream_useds,\n",
    "            severity_deltas = severity_deltas,\n",
    "            learning_rate=1e-3,\n",
    "            max_grad_steps=30\n",
    "        )\n",
    "\n",
    "        # now explicitly get the scoring vector\n",
    "        last_state_weights = np.array(updated_parameters[\"scoring\"], dtype = float)\n",
    "        last_state_biases = np.array(updated_parameters[\"biases\"], dtype = float)\n",
    "\n",
    "        # Update scoring in model_config for next iteration\n",
    "        model_config[\"scoring\"] = last_state_weights\n",
    "        model_config[\"biases\"] = last_state_biases\n",
    "        \n",
    "    print(last_params)\n",
    "    return v_predictions, last_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "2d8ce21b-86ef-42ba-b8f7-afec8d847419",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to view the data? Type Yes or No. No\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, going on to modeling.\n",
      "ok\n",
      "{'r_growth': np.float64(0.9999999999999996), 'K_CC': 1.0, 'k_antibiotics': np.float64(0.18420681743952366), 'k_sebum': np.float64(0.18420681743952366), 'I_bacterial_induction': np.float64(0.18420681743952366), 'I_decay_tstd': np.float64(0.18420681743952366), 'I_baseline_decay': np.float64(0.18420681743952366), 'r_I_production': np.float64(0.18420681743952366), 'r_cream_clean': np.float64(0.18420681743952366), 'noise': 0.01, 'w_sigma': 0.3, 'm_sigma': 0.5}\n"
     ]
    }
   ],
   "source": [
    "def main(this_raw_data_name, json_name):\n",
    "    data_returns = data_parsing(this_raw_data_name)\n",
    "    #this_data_visualization = data_visualization(data_returns[1], data_returns[0], data_returns[4], data_returns[3])\n",
    "    these_ranges = data_returns[3]\n",
    "    these_averages = data_returns[5]\n",
    "    these_dirichlets = data_returns[6]\n",
    "\n",
    "    view_plots = False\n",
    "    user_input = input(\"Do you want to view the data? Type Yes or No.\")\n",
    "    \n",
    "    if user_input == \"Yes\":\n",
    "         view_plots = True\n",
    "    if view_plots:\n",
    "        \n",
    "        this_data_visualization = data_visualization(data_returns[1], data_returns[0], data_returns[4], data_returns[3])\n",
    "    if not view_plots:\n",
    "        print(\"Ok, going on to modeling.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open(json_name, \"r\") as icgs:\n",
    "        initial_constant_guesses = json.load(icgs)\n",
    "        this_model_config = {\"scoring\": np.random.randn(3, 3) * 0.02,\n",
    "                        #column order: low severity change, medium, high. row order: #bacteria, inflammation, sebum.\n",
    "    \"biases\": [0, 0, 0],\n",
    "    \"Q\": np.eye(3) * (initial_constant_guesses[\"w_sigma\"]**2),\n",
    "    \"R\": np.eye(3) * (initial_constant_guesses[\"m_sigma\"]**2)\n",
    "\n",
    "}\n",
    "           \n",
    "        \n",
    "    this_built_model = model_building(data_returns[1], initial_constant_guesses, these_dirichlets, these_averages, this_model_config)\n",
    "    \n",
    "    \n",
    "\n",
    "#retrieving Acne04 dataset from Kaggle source (local download on machine)\n",
    "this_raw_data_name = \"sim_acne.csv\"\n",
    "if __name__ == \"__main__\":\n",
    "    main(this_raw_data_name, \"initial_constants.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "27484e4b-a5be-492b-bd50-a9d4c87ffc4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_piecewise_regression_and_plot_unused(points, splits):\n",
    "    \"\"\"This function actually splits a given array (here, cumulative KL divergence) into seperate regression models,\n",
    "    using splits to partition the array and then fit a linear regression model to each one.\n",
    "    It then calls plot_piecewise_regression_segments to plot the segments over the cumulative KL divergence Curve. Unused in this version.\"\"\"\n",
    "    split_points_and_indices = [(points[split[0]:split[1]+1], split) for split in splits]\n",
    "    linear_model = LinearRegression()\n",
    "    consecutive_models = [LinearRegression().fit(np.arange(len(one_split_points[0])).reshape(-1, 1),one_split_points[0].reshape(-1, 1))for one_split_points in split_points_and_indices]\n",
    "\n",
    "    slopes = [which_model.coef_[0] for which_model in consecutive_models]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
